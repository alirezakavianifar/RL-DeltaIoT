{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4vv2sF5dVTqUN5LgJwLOf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezakavianifar/RL-DeltaIoT/blob/main/Innovativeideas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EmKcAIHYu5Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a reinforcement learning (RL) environment for optimizing energy consumption in a software system using Gymnasium involves several steps. Below is a basic implementation to get you started. This example assumes you have a function that can evaluate the energy consumption given a configuration.\n",
        "\n",
        "First, install Gymnasium if you haven't already:\n",
        "\n",
        "```bash\n",
        "pip install gymnasium\n",
        "```\n",
        "\n",
        "Next, create the RL environment. The `SoftwareEnv` class below represents a Gymnasium environment for your software system:\n",
        "\n",
        "```python\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "class SoftwareEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, config_space, energy_function):\n",
        "        super(SoftwareEnv, self).__init__()\n",
        "        \n",
        "        # Configuration space (list of lists, where each list represents the range of values for a feature)\n",
        "        self.config_space = config_space\n",
        "        \n",
        "        # Action and observation space\n",
        "        self.action_space = spaces.MultiDiscrete([len(feature_space) for feature_space in config_space])\n",
        "        self.observation_space = spaces.MultiDiscrete([len(feature_space) for feature_space in config_space])\n",
        "        \n",
        "        # Energy consumption evaluation function\n",
        "        self.energy_function = energy_function\n",
        "        \n",
        "        # Current configuration\n",
        "        self.state = self._get_random_config()\n",
        "    \n",
        "    def _get_random_config(self):\n",
        "        return [np.random.choice(len(feature_space)) for feature_space in self.config_space]\n",
        "    \n",
        "    def reset(self):\n",
        "        self.state = self._get_random_config()\n",
        "        return self.state\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.state = action\n",
        "        \n",
        "        # Calculate energy consumption based on the current configuration\n",
        "        energy_consumption = self.energy_function(self.state)\n",
        "        \n",
        "        # For optimization, we can use the negative energy consumption as the reward\n",
        "        reward = -energy_consumption\n",
        "        \n",
        "        done = False\n",
        "        \n",
        "        return self.state, reward, done, {}\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        print(f'Current Configuration: {self.state}')\n",
        "        \n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "# Example energy consumption function\n",
        "def example_energy_function(config):\n",
        "    # Dummy function: sum of the configuration vector\n",
        "    return sum(config)\n",
        "\n",
        "# Example configuration space: each feature has a range from 0 to 4\n",
        "config_space = [list(range(5)), list(range(5)), list(range(5))]\n",
        "\n",
        "# Create the environment\n",
        "env = SoftwareEnv(config_space, example_energy_function)\n",
        "\n",
        "# Example usage\n",
        "state = env.reset()\n",
        "print(f'Initial state: {state}')\n",
        "\n",
        "action = [1, 2, 3]\n",
        "state, reward, done, info = env.step(action)\n",
        "print(f'State: {state}, Reward: {reward}')\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "1. **Environment Initialization**:\n",
        "    - `config_space`: Defines the possible values for each feature in the configuration vector.\n",
        "    - `action_space` and `observation_space`: Define the spaces using `spaces.MultiDiscrete` based on the `config_space`.\n",
        "    - `energy_function`: A function that computes the energy consumption given a configuration.\n",
        "\n",
        "2. **State Initialization**:\n",
        "    - `_get_random_config()`: Generates a random initial configuration.\n",
        "\n",
        "3. **Step Function**:\n",
        "    - Takes an action (new configuration) and calculates the new state.\n",
        "    - Computes the energy consumption and returns the negative as the reward.\n",
        "    - In this simple version, `done` is always `False` since there's no terminal state defined.\n",
        "\n",
        "4. **Example Energy Function**:\n",
        "    - A simple example where the energy consumption is the sum of the configuration values.\n",
        "\n",
        "5. **Environment Interaction**:\n",
        "    - Resetting the environment to get an initial state.\n",
        "    - Taking an action and receiving the next state, reward, and other info.\n",
        "\n",
        "### Using the Environment with RL Algorithms:\n",
        "\n",
        "To train an RL agent using this environment, you can use libraries such as Stable Baselines3. Here is an example of how to use Stable Baselines3 with this custom environment:\n",
        "\n",
        "```bash\n",
        "pip install stable-baselines3\n",
        "```\n",
        "\n",
        "```python\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Create the environment\n",
        "env = SoftwareEnv(config_space, example_energy_function)\n",
        "\n",
        "# Initialize the agent\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Save the agent\n",
        "model.save(\"ppo_software_env\")\n",
        "\n",
        "# Load the agent\n",
        "model = PPO.load(\"ppo_software_env\")\n",
        "\n",
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "for _ in range(100):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.render()\n",
        "```\n",
        "\n",
        "This script sets up the PPO algorithm to train on the custom environment and then tests the trained agent. You can adjust the `total_timesteps` and other parameters to fit your specific needs."
      ],
      "metadata": {
        "id": "LTHeoqtVY6WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reward function in a reinforcement learning environment is crucial for guiding the agent towards the desired goal. In the context of optimizing energy consumption, the reward function should reflect the efficiency of the configuration in terms of energy usage. Here are a few ideas for designing the reward function:\n",
        "\n",
        "1. **Negative Energy Consumption**:\n",
        "    - The simplest approach is to use the negative value of the energy consumption as the reward. This way, lower energy consumption results in a higher reward.\n",
        "    ```python\n",
        "    reward = -energy_consumption\n",
        "    ```\n",
        "\n",
        "2. **Energy Consumption with Penalties**:\n",
        "    - If there are additional constraints or goals (e.g., performance metrics that should not be compromised), you can incorporate penalties into the reward function.\n",
        "    ```python\n",
        "    performance_metric = evaluate_performance(config)  # Example performance evaluation\n",
        "    penalty = 0\n",
        "    if performance_metric < threshold:\n",
        "        penalty = large_value  # Penalty for not meeting performance criteria\n",
        "    reward = -energy_consumption - penalty\n",
        "    ```\n",
        "\n",
        "3. **Normalized Energy Consumption**:\n",
        "    - Normalizing the energy consumption within a certain range can help in situations where the absolute values vary significantly.\n",
        "    ```python\n",
        "    max_energy = 100  # Example maximum energy consumption\n",
        "    min_energy = 0    # Example minimum energy consumption\n",
        "    reward = -((energy_consumption - min_energy) / (max_energy - min_energy))\n",
        "    ```\n",
        "\n",
        "4. **Relative Improvement**:\n",
        "    - Reward based on the improvement over a baseline or previous state. This can encourage continuous improvement.\n",
        "    ```python\n",
        "    if previous_energy_consumption is not None:\n",
        "        improvement = previous_energy_consumption - energy_consumption\n",
        "        reward = improvement\n",
        "    else:\n",
        "        reward = -energy_consumption\n",
        "    previous_energy_consumption = energy_consumption\n",
        "    ```\n",
        "\n",
        "Here’s how you might integrate a simple negative energy consumption reward into the `SoftwareEnv` class:\n",
        "\n",
        "```python\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "class SoftwareEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, config_space, energy_function):\n",
        "        super(SoftwareEnv, self).__init__()\n",
        "        \n",
        "        # Configuration space (list of lists, where each list represents the range of values for a feature)\n",
        "        self.config_space = config_space\n",
        "        \n",
        "        # Action and observation space\n",
        "        self.action_space = spaces.MultiDiscrete([len(feature_space) for feature_space in config_space])\n",
        "        self.observation_space = spaces.MultiDiscrete([len(feature_space) for feature_space in config_space])\n",
        "        \n",
        "        # Energy consumption evaluation function\n",
        "        self.energy_function = energy_function\n",
        "        \n",
        "        # Current configuration\n",
        "        self.state = self._get_random_config()\n",
        "    \n",
        "    def _get_random_config(self):\n",
        "        return [np.random.choice(len(feature_space)) for feature_space in self.config_space]\n",
        "    \n",
        "    def reset(self):\n",
        "        self.state = self._get_random_config()\n",
        "        return self.state\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.state = action\n",
        "        \n",
        "        # Calculate energy consumption based on the current configuration\n",
        "        energy_consumption = self.energy_function(self.state)\n",
        "        \n",
        "        # Reward function: Negative of the energy consumption\n",
        "        reward = -energy_consumption\n",
        "        \n",
        "        # Assuming no terminal state for simplicity\n",
        "        done = False\n",
        "        \n",
        "        return self.state, reward, done, {}\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        print(f'Current Configuration: {self.state}, Energy Consumption: {self.energy_function(self.state)}')\n",
        "        \n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "# Example energy consumption function\n",
        "def example_energy_function(config):\n",
        "    # Dummy function: sum of the configuration vector\n",
        "    return sum(config)\n",
        "\n",
        "# Example configuration space: each feature has a range from 0 to 4\n",
        "config_space = [list(range(5)), list(range(5)), list(range(5))]\n",
        "\n",
        "# Create the environment\n",
        "env = SoftwareEnv(config_space, example_energy_function)\n",
        "\n",
        "# Example usage\n",
        "state = env.reset()\n",
        "print(f'Initial state: {state}')\n",
        "\n",
        "action = [1, 2, 3]\n",
        "state, reward, done, info = env.step(action)\n",
        "print(f'State: {state}, Reward: {reward}')\n",
        "```\n",
        "\n",
        "### Explanation of the Reward Function:\n",
        "- The reward is set to the negative of the energy consumption (`reward = -energy_consumption`). This means that lower energy consumption will result in a higher (less negative) reward.\n",
        "- The `step` function updates the environment state based on the action, calculates the energy consumption for the new configuration, computes the reward, and returns the new state, reward, and other info.\n",
        "\n",
        "### Further Refinements:\n",
        "- You can incorporate more sophisticated reward mechanisms depending on additional factors like performance metrics, penalties for constraint violations, or incentives for improvements. Adjusting the reward function according to the specific goals and constraints of your system will help the RL agent learn more effectively."
      ],
      "metadata": {
        "id": "jglS1sMSZHp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The related work section of the document you provided discusses various strategies and approaches for software configuration tuning, focusing on the challenges and methodologies in this domain. Below is a summary of the relevant points:\n",
        "\n",
        "### Overview of Software Configuration Tuning\n",
        "Software configuration tuning is essential for optimizing performance objectives, such as minimizing latency or maximizing throughput. The complexity and expense of measuring software configurations have historically led to limited success in this field, particularly in avoiding local optima traps.\n",
        "\n",
        "### Traditional Optimization Approaches\n",
        "Several traditional optimization approaches have been employed in software configuration tuning, including:\n",
        "- **Random Search:** Simple yet often ineffective in complex landscapes due to lack of directionality.\n",
        "- **Hill Climbing:** Focuses on local improvements, which can lead to local optima.\n",
        "- **Genetic Algorithms:** Use evolutionary strategies to explore the search space but may still face challenges with local optima.\n",
        "- **Simulated Annealing:** Balances exploration and exploitation but requires careful tuning of parameters.\n",
        "\n",
        "These approaches typically focus on the internal components of the optimizer, such as search operators and strategies, to improve performance.\n",
        "\n",
        "### Multi-Objectivization Approach\n",
        "Multi-objectivization transforms a single-objective optimization problem into a multi-objective one to avoid local optima by making similarly performing configurations less comparable (i.e., Pareto nondominated). This method aims to facilitate better exploration of the search space and avoid local optima by introducing auxiliary objectives.\n",
        "\n",
        "#### MMO Model\n",
        "The MMO (Meta Multi-Objectivization) model introduces an auxiliary performance objective in addition to the primary one. Instead of optimizing this auxiliary objective, it serves to create a diverse set of configurations that are Pareto nondominated, thus preventing the search from being trapped in local optima. The key innovation here is a new normalization method that effectively uses the MMO model without requiring sensitive parameter tuning.\n",
        "\n",
        "### Evaluation and Results\n",
        "Experiments on various real-world software systems demonstrate that the MMO model with the new normalization method outperforms traditional single-objective optimizers and achieves significant speedup. The MMO model also shows improvements over model-based tuning methods like FLASH and BOCA.\n",
        "\n",
        "### Contributions\n",
        "The main contributions of the discussed work are:\n",
        "- Introduction of the MMO model for better handling of local optima in software configuration tuning.\n",
        "- Development of a new normalization method to eliminate the need for sensitive parameter tuning.\n",
        "- Extensive evaluation demonstrating the effectiveness and efficiency of the MMO model across multiple systems and environments.\n",
        "\n",
        "The related work section sets the stage for understanding the challenges and advancements in the field of software configuration tuning, highlighting the novel approach of multi-objectivization for improved performance optimization."
      ],
      "metadata": {
        "id": "uyj2RkoZZP_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of the document you provided, a normalization method refers to a technique used to adjust the scales of different objectives to make them comparable and manageable within the optimization process. Normalization is especially important in multi-objective optimization, where different objectives might have different units or ranges, making direct comparisons difficult.\n",
        "\n",
        "### Purpose of Normalization\n",
        "\n",
        "1. **Scale Adjustment**:\n",
        "   - Different objectives in an optimization problem can have varying scales. For example, energy consumption might range from 0 to 1000 units, while latency might range from 0 to 10 milliseconds. Normalization adjusts these scales to a common range, typically [0, 1], to ensure that no single objective disproportionately influences the optimization process.\n",
        "\n",
        "2. **Facilitation of Pareto Optimization**:\n",
        "   - In multi-objective optimization, solutions are often evaluated based on Pareto dominance, where one solution is considered better if it is no worse in all objectives and strictly better in at least one. Normalization helps in comparing the performance across different objectives by bringing them to a common scale.\n",
        "\n",
        "3. **Improving Convergence**:\n",
        "   - Normalization can help in guiding the optimization algorithm more effectively, especially in complex search spaces, by providing a balanced view of improvements across multiple objectives.\n",
        "\n",
        "### Normalization in the MMO Model\n",
        "\n",
        "The MMO (Meta Multi-Objectivization) model introduced in the document uses normalization as a key component to handle the auxiliary objectives effectively. Here’s how it works in the context of the MMO model:\n",
        "\n",
        "1. **Auxiliary Performance Objective**:\n",
        "   - The MMO model introduces an auxiliary performance objective alongside the primary one. This auxiliary objective helps create a diverse set of configurations that are Pareto nondominated, aiding in better exploration of the search space.\n",
        "\n",
        "2. **Normalization Method**:\n",
        "   - The new normalization method developed in the MMO model helps adjust the scales of the primary and auxiliary objectives to make them comparable. This normalization method eliminates the need for sensitive parameter tuning, which is often a challenge in traditional optimization methods.\n",
        "   \n",
        "3. **Parameter-Free Optimization**:\n",
        "   - By effectively normalizing the objectives, the MMO model can operate without requiring the user to carefully tune parameters for balancing different objectives. This makes the optimization process more robust and easier to apply across different scenarios.\n",
        "\n",
        "### Example of Normalization\n",
        "\n",
        "A typical normalization formula for an objective \\( O \\) could be:\n",
        "\n",
        "\\[ O_{\\text{normalized}} = \\frac{O - O_{\\text{min}}}{O_{\\text{max}} - O_{\\text{min}}} \\]\n",
        "\n",
        "where \\( O_{\\text{min}} \\) and \\( O_{\\text{max}} \\) are the minimum and maximum observed values of the objective \\( O \\). This scales \\( O \\) to a range of [0, 1].\n",
        "\n",
        "### Impact of Normalization\n",
        "\n",
        "Normalization impacts the optimization process by ensuring that each objective is weighted equally in terms of its influence on the decision-making process. This is crucial in multi-objective optimization to avoid scenarios where one objective dominates the others purely due to its scale.\n",
        "\n",
        "In summary, the normalization method in the MMO model adjusts the scales of different objectives to a common range, making them comparable and facilitating effective Pareto optimization. This leads to improved exploration of the search space and better overall performance of the optimization algorithm without requiring extensive parameter tuning."
      ],
      "metadata": {
        "id": "Q_7RpRVAZRSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normalization method introduced in the paper is designed to address issues found in the FSE work. The new normalization method adjusts the objective values based on the local bounds of the current population of configurations instead of global bounds. This method ensures that the values of both the target and auxiliary objectives are commensurable, which reduces the need for fine-tuning a weight parameter and helps maintain a balance between selection pressure and diversity.\n",
        "\n",
        "Here is a summary of the key aspects of the new normalization method:\n",
        "\n",
        "1. **Local Bounds Normalization**: The normalization is based on the minimum and maximum values of the current population for each generation. This approach adapts dynamically to the evolving population during the optimization process.\n",
        "   \n",
        "   The normalization formula used is:\n",
        "   \\[\n",
        "   f(x) = \\frac{f_o(x) - f_o^{\\min}}{f_o^{\\max} - f_o^{\\min}}\n",
        "   \\]\n",
        "   where \\( f_o(x) \\) is the original objective value of configuration \\( x \\), and \\( f_o^{\\min} \\) and \\( f_o^{\\max} \\) are the minimum and maximum values of the current population for the objective \\( f \\)【15:3†source】.\n",
        "\n",
        "2. **Elimination of Weight Parameter**: By using this normalization method, the need for a weight parameter \\( w \\) to balance the target and auxiliary objectives is removed. This makes the optimization process simpler and more robust as it eliminates the risk of poor outcomes due to inappropriate weight settings【15:4†source】.\n",
        "\n",
        "3. **Improved Effectiveness**: The new normalization method has been shown to lead to better results compared to the previous normalization method used in the FSE work. It achieves a good balance between imposing selection pressure towards the best target performance and preserving diversity in auxiliary performance【15:3†source】【15:4†source】.\n",
        "\n",
        "4. **Pseudo-code Integration**: The pseudo-code for the modified MMO model with the new normalization method integrates these adjustments by updating the bounds at each generation and normalizing the objective values accordingly. This is reflected in lines 6-7 and 23-24 of the pseudo-code in the document【15:0†source】【15:2†source】.\n",
        "\n",
        "By adapting the normalization dynamically based on the local bounds, the new method ensures that the configurations do not concentrate into a single value on either objective, thus maintaining a diverse and effective search space throughout the optimization process【15:1†source】."
      ],
      "metadata": {
        "id": "ONslBBLTZSdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hindsight Experience Replay (HER) is an effective technique originally proposed for goal-oriented reinforcement learning tasks. It involves storing trajectories where the goals are replaced with states that were actually achieved, allowing the agent to learn from both successful and unsuccessful experiences by treating failures as intentional successes towards different goals. Adapting HER for software configuration tuning could involve redefining goals and leveraging past experiences to enhance optimization.\n",
        "\n",
        "Here's a step-by-step outline to propose a method that uses HER for the same purpose as stated in the article (i.e., optimizing software configurations):\n",
        "\n",
        "### Step 1: Define Goals and States\n",
        "- **Goals**: In the context of software configuration tuning, goals can be considered as target configurations that lead to optimal performance in terms of energy consumption or other metrics.\n",
        "- **States**: Each state represents a specific configuration of the software system.\n",
        "\n",
        "### Step 2: Adapt the Reward Function\n",
        "- The reward function should reflect the performance (e.g., energy consumption) of the configurations. Negative energy consumption can be used as the reward, as previously discussed.\n",
        "\n",
        "### Step 3: Implement Hindsight Experience Replay\n",
        "HER can be integrated into the RL algorithm to augment the learning process with experiences that might have been failures but can be treated as successful outcomes for different goals.\n",
        "\n",
        "#### Pseudo-code Implementation\n",
        "\n",
        "Here’s a simplified pseudo-code that integrates HER into the training loop of an RL algorithm for software configuration tuning:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class HERSoftwareEnv(SoftwareEnv):\n",
        "    def __init__(self, config_space, energy_function, her_k=4):\n",
        "        super(HERSoftwareEnv, self).__init__(config_space, energy_function)\n",
        "        self.her_k = her_k\n",
        "        self.memory = deque(maxlen=10000)  # Replay buffer\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.state = action\n",
        "        energy_consumption = self.energy_function(self.state)\n",
        "        reward = -energy_consumption\n",
        "        done = False\n",
        "        return self.state, reward, done, {}\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample_memory(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def hindsight_replay(self):\n",
        "        her_batch = []\n",
        "        for state, action, reward, next_state, done in self.memory:\n",
        "            her_goals = self._sample_her_goals(state)\n",
        "            for goal in her_goals:\n",
        "                her_reward = self._compute_reward(state, goal)\n",
        "                her_batch.append((state, action, her_reward, goal, done))\n",
        "        self.memory.extend(her_batch)\n",
        "    \n",
        "    def _sample_her_goals(self, state):\n",
        "        # Sample k random states from the future trajectory as goals\n",
        "        future_states = [random.choice(self.memory)[3] for _ in range(self.her_k)]\n",
        "        return future_states\n",
        "    \n",
        "    def _compute_reward(self, state, goal):\n",
        "        # Use the energy consumption of the goal state as the reward\n",
        "        energy_consumption = self.energy_function(goal)\n",
        "        return -energy_consumption\n",
        "\n",
        "# Example usage\n",
        "config_space = [list(range(5)), list(range(5)), list(range(5))]\n",
        "def example_energy_function(config):\n",
        "    return sum(config)\n",
        "\n",
        "env = HERSoftwareEnv(config_space, example_energy_function)\n",
        "\n",
        "# Train the agent with HER\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Replace with a policy action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        env.store_transition(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "    \n",
        "    env.hindsight_replay()  # Augment the replay buffer with HER\n",
        "\n",
        "# Example of sampling from memory for training\n",
        "batch_size = 64\n",
        "samples = env.sample_memory(batch_size)\n",
        "```\n",
        "\n",
        "### Step 4: Integrate with a RL Algorithm\n",
        "- The HER-augmented experiences can be used to train the RL agent. Libraries like Stable Baselines3 can be modified to incorporate HER by customizing the replay buffer and training loop.\n",
        "\n",
        "### Step 5: Evaluate and Iterate\n",
        "- Evaluate the performance of the agent with and without HER to assess the effectiveness of the hindsight experiences.\n",
        "- Iterate on the approach by tuning parameters like the number of hindsight goals (her_k) and the structure of the reward function.\n",
        "\n",
        "### Explanation of Key Components\n",
        "- **Replay Buffer**: Stores experiences (state, action, reward, next_state, done) and is augmented with hindsight experiences.\n",
        "- **Hindsight Goals**: For each experience, sample a few future states and treat them as goals.\n",
        "- **Hindsight Rewards**: Recompute rewards based on the new goals, typically using the energy consumption of these goals.\n",
        "\n",
        "By leveraging HER, the agent can learn more efficiently from a richer set of experiences, potentially leading to better performance in optimizing software configurations."
      ],
      "metadata": {
        "id": "l70l3CLQZkEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, let's walk through a concrete example of how Hindsight Experience Replay (HER) could be used to minimize energy consumption in software configuration tuning.\n",
        "\n",
        "### Example Scenario\n",
        "Imagine we have a software system with multiple configuration parameters, each affecting the system's energy consumption. Our goal is to find configurations that minimize energy consumption.\n",
        "\n",
        "### Key Components\n",
        "- **State**: A specific configuration of the software system.\n",
        "- **Action**: Adjusting the configuration parameters.\n",
        "- **Reward**: Negative energy consumption (lower energy consumption results in a higher reward).\n",
        "\n",
        "### Steps to Implement HER\n",
        "\n",
        "#### 1. Define the Environment and Reward Function\n",
        "First, define the environment and the energy consumption function:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class HERSoftwareEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, config_space, energy_function, her_k=4):\n",
        "        super(HERSoftwareEnv, self).__init__()\n",
        "        \n",
        "        # Configuration space\n",
        "        self.config_space = config_space\n",
        "        self.action_space = spaces.MultiDiscrete([len(feature_space) for feature_space in config_space])\n",
        "        self.observation_space = spaces.MultiDiscrete([len(feature_space) for feature_space in config_space])\n",
        "        \n",
        "        # Energy consumption function\n",
        "        self.energy_function = energy_function\n",
        "        \n",
        "        # HER parameters\n",
        "        self.her_k = her_k\n",
        "        self.memory = deque(maxlen=10000)  # Replay buffer\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.state = self._get_random_config()\n",
        "        return self.state\n",
        "    \n",
        "    def _get_random_config(self):\n",
        "        return [np.random.choice(len(feature_space)) for feature_space in self.config_space]\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.state = action\n",
        "        energy_consumption = self.energy_function(self.state)\n",
        "        reward = -energy_consumption\n",
        "        done = False\n",
        "        return self.state, reward, done, {}\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample_memory(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def hindsight_replay(self):\n",
        "        her_batch = []\n",
        "        for state, action, reward, next_state, done in self.memory:\n",
        "            her_goals = self._sample_her_goals(state)\n",
        "            for goal in her_goals:\n",
        "                her_reward = self._compute_reward(goal)\n",
        "                her_batch.append((state, action, her_reward, goal, done))\n",
        "        self.memory.extend(her_batch)\n",
        "    \n",
        "    def _sample_her_goals(self, state):\n",
        "        # Sample k random states from the future trajectory as goals\n",
        "        future_states = [random.choice(self.memory)[3] for _ in range(self.her_k)]\n",
        "        return future_states\n",
        "    \n",
        "    def _compute_reward(self, goal):\n",
        "        # Use the energy consumption of the goal state as the reward\n",
        "        energy_consumption = self.energy_function(goal)\n",
        "        return -energy_consumption\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        print(f'Current Configuration: {self.state}, Energy Consumption: {self.energy_function(self.state)}')\n",
        "\n",
        "# Example energy consumption function\n",
        "def example_energy_function(config):\n",
        "    return sum(config)  # Dummy function: sum of the configuration vector\n",
        "\n",
        "# Example configuration space: each feature has a range from 0 to 4\n",
        "config_space = [list(range(5)), list(range(5)), list(range(5))]\n",
        "\n",
        "# Create the environment\n",
        "env = HERSoftwareEnv(config_space, example_energy_function)\n",
        "```\n",
        "\n",
        "#### 2. Integrate HER into Training\n",
        "Train an RL agent with HER:\n",
        "\n",
        "```python\n",
        "# Example usage with HER\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Replace with a policy action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        env.store_transition(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "    \n",
        "    env.hindsight_replay()  # Augment the replay buffer with HER\n",
        "\n",
        "# Example of sampling from memory for training\n",
        "batch_size = 64\n",
        "samples = env.sample_memory(batch_size)\n",
        "```\n",
        "\n",
        "### Detailed Example of HER in Action\n",
        "\n",
        "#### Initial Configuration and Trajectory\n",
        "\n",
        "1. **Episode Start**: Start with an initial random configuration, e.g., `[0, 2, 3]`.\n",
        "2. **Actions and Transitions**:\n",
        "    - Action: Change to `[1, 2, 3]`, reward = -6 (energy consumption = 6).\n",
        "    - Action: Change to `[1, 3, 3]`, reward = -7 (energy consumption = 7).\n",
        "    - Action: Change to `[2, 3, 4]`, reward = -9 (energy consumption = 9).\n",
        "\n",
        "#### Hindsight Experience Replay (HER)\n",
        "\n",
        "During HER, we sample future states as goals:\n",
        "\n",
        "- Original experience: `(state, action, reward, next_state, done)`.\n",
        "- Suppose `[2, 3, 4]` is chosen as a goal from future states.\n",
        "\n",
        "For each transition, we recompute the reward as if the goal was to reach `[2, 3, 4]`:\n",
        "\n",
        "1. Transition: `[0, 2, 3] -> [1, 2, 3]` with hindsight goal `[2, 3, 4]`.\n",
        "    - Hindsight reward: Compute based on `[2, 3, 4]`, e.g., energy consumption = 9, reward = -9.\n",
        "\n",
        "2. Transition: `[1, 2, 3] -> [1, 3, 3]` with hindsight goal `[2, 3, 4]`.\n",
        "    - Hindsight reward: Compute based on `[2, 3, 4]`, e.g., energy consumption = 9, reward = -9.\n",
        "\n",
        "3. Transition: `[1, 3, 3] -> [2, 3, 4]` with hindsight goal `[2, 3, 4]`.\n",
        "    - Hindsight reward: Compute based on `[2, 3, 4]`, e.g., energy consumption = 9, reward = -9.\n",
        "\n",
        "By using HER, the agent learns from both the original and hindsight experiences, effectively augmenting the training data and improving its ability to find configurations that minimize energy consumption. This approach allows the agent to generalize better and learn more efficiently by treating unsuccessful trajectories as successful ones for different goals."
      ],
      "metadata": {
        "id": "eG7TOyohZpdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Hindsight Experience Replay (HER) to minimize energy consumption while using packet loss as a hindsight goal is a creative approach to explore the search space more effectively. Here’s how you can implement this:\n",
        "\n",
        "### Concept\n",
        "- **Primary Goal**: Minimize energy consumption.\n",
        "- **Hindsight Goal**: Minimize packet loss.\n",
        "\n",
        "By treating packet loss as an auxiliary goal, we can encourage the agent to explore configurations that might not immediately seem optimal for energy consumption but could lead to better overall configurations when considering both objectives.\n",
        "\n",
        "### Steps to Implement HER with Packet Loss\n",
        "\n",
        "#### 1. Modify the Environment to Include Packet Loss\n",
        "Extend the environment to compute both energy consumption and packet loss.\n",
        "\n",
        "```python\n",
        "class HERSoftwareEnvWithPacketLoss(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, config_space, energy_function, packet_loss_function, her_k=4):\n",
        "        super(HERSoftwareEnvWithPacketLoss, self).__init__()\n",
        "        \n",
        "        # Configuration space\n",
        "        self.config_space = config_space\n",
        "        self.action_space = spaces.MultiDiscrete([len(feature_space) for feature_space in config_space])\n",
        "        self.observation_space = spaces.MultiDiscrete([len(feature_space) for feature_space in config_space])\n",
        "        \n",
        "        # Performance functions\n",
        "        self.energy_function = energy_function\n",
        "        self.packet_loss_function = packet_loss_function\n",
        "        \n",
        "        # HER parameters\n",
        "        self.her_k = her_k\n",
        "        self.memory = deque(maxlen=10000)  # Replay buffer\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.state = self._get_random_config()\n",
        "        return self.state\n",
        "    \n",
        "    def _get_random_config(self):\n",
        "        return [np.random.choice(len(feature_space)) for feature_space in self.config_space]\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.state = action\n",
        "        energy_consumption = self.energy_function(self.state)\n",
        "        packet_loss = self.packet_loss_function(self.state)\n",
        "        reward = -energy_consumption  # Primary goal: minimize energy consumption\n",
        "        done = False\n",
        "        return self.state, reward, done, {'packet_loss': packet_loss}\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done, info):\n",
        "        self.memory.append((state, action, reward, next_state, done, info))\n",
        "    \n",
        "    def sample_memory(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def hindsight_replay(self):\n",
        "        her_batch = []\n",
        "        for state, action, reward, next_state, done, info in self.memory:\n",
        "            her_goals = self._sample_her_goals()\n",
        "            for goal in her_goals:\n",
        "                her_reward = self._compute_hindsight_reward(goal)\n",
        "                her_info = {'packet_loss': self.packet_loss_function(goal)}\n",
        "                her_batch.append((state, action, her_reward, goal, done, her_info))\n",
        "        self.memory.extend(her_batch)\n",
        "    \n",
        "    def _sample_her_goals(self):\n",
        "        # Sample k random future states as goals\n",
        "        future_states = [random.choice(self.memory)[3] for _ in range(self.her_k)]\n",
        "        return future_states\n",
        "    \n",
        "    def _compute_hindsight_reward(self, goal):\n",
        "        # Use the packet loss of the goal state as the hindsight reward\n",
        "        packet_loss = self.packet_loss_function(goal)\n",
        "        return -packet_loss  # Hindsight goal: minimize packet loss\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        print(f'Current Configuration: {self.state}, Energy Consumption: {self.energy_function(self.state)}, Packet Loss: {self.packet_loss_function(self.state)}')\n",
        "\n",
        "# Example energy and packet loss functions\n",
        "def example_energy_function(config):\n",
        "    return sum(config)  # Dummy function: sum of the configuration vector\n",
        "\n",
        "def example_packet_loss_function(config):\n",
        "    return max(config)  # Dummy function: max of the configuration vector\n",
        "\n",
        "# Example configuration space: each feature has a range from 0 to 4\n",
        "config_space = [list(range(5)), list(range(5)), list(range(5))]\n",
        "\n",
        "# Create the environment\n",
        "env = HERSoftwareEnvWithPacketLoss(config_space, example_energy_function, example_packet_loss_function)\n",
        "```\n",
        "\n",
        "#### 2. Train the Agent with HER Using Packet Loss as Hindsight Goal\n",
        "In the training loop, store transitions and perform hindsight replay.\n",
        "\n",
        "```python\n",
        "# Example usage with HER\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Replace with a policy action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        env.store_transition(state, action, reward, next_state, done, info)\n",
        "        state = next_state\n",
        "    \n",
        "    env.hindsight_replay()  # Augment the replay buffer with HER\n",
        "\n",
        "# Example of sampling from memory for training\n",
        "batch_size = 64\n",
        "samples = env.sample_memory(batch_size)\n",
        "```\n",
        "\n",
        "### Detailed Example of HER with Packet Loss\n",
        "\n",
        "1. **Initial Configuration and Trajectory**:\n",
        "    - Start with an initial random configuration, e.g., `[0, 2, 3]`.\n",
        "    - Action: Change to `[1, 2, 3]`, reward = -6 (energy consumption), packet loss = 3.\n",
        "    - Action: Change to `[1, 3, 3]`, reward = -7 (energy consumption), packet loss = 3.\n",
        "    - Action: Change to `[2, 3, 4]`, reward = -9 (energy consumption), packet loss = 4.\n",
        "\n",
        "2. **Hindsight Experience Replay**:\n",
        "    - Original experience: `(state, action, reward, next_state, done, info)`.\n",
        "    - Suppose `[2, 3, 4]` is chosen as a hindsight goal from future states.\n",
        "\n",
        "3. **Recompute Hindsight Reward**:\n",
        "    - Transition: `[0, 2, 3] -> [1, 2, 3]` with hindsight goal `[2, 3, 4]`.\n",
        "        - Hindsight reward: Compute based on packet loss of `[2, 3, 4]`, e.g., packet loss = 4, reward = -4.\n",
        "\n",
        "    - Transition: `[1, 2, 3] -> [1, 3, 3]` with hindsight goal `[2, 3, 4]`.\n",
        "        - Hindsight reward: Compute based on packet loss of `[2, 3, 4]`, e.g., packet loss = 4, reward = -4.\n",
        "\n",
        "    - Transition: `[1, 3, 3] -> [2, 3, 4]` with hindsight goal `[2, 3, 4]`.\n",
        "        - Hindsight reward: Compute based on packet loss of `[2, 3, 4]`, e.g., packet loss = 4, reward = -4.\n",
        "\n",
        "By using HER with packet loss as the hindsight goal, the agent learns from both the original and hindsight experiences. This helps in exploring configurations that might initially seem suboptimal for energy consumption but could lead to better overall configurations by considering packet loss. This approach encourages exploration and helps the agent find configurations that balance both energy consumption and packet loss."
      ],
      "metadata": {
        "id": "ESZ-gsNYZuvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, implementing a normalization method can indeed help the agent learn better when dealing with objectives that have different scales, such as energy consumption and packet loss. Normalizing the objectives to a common scale ensures that neither objective disproportionately influences the learning process.\n",
        "\n",
        "### Implementing Normalization\n",
        "\n",
        "You can normalize the objectives based on their respective minimum and maximum values observed during the training process. Here's an approach to normalize both energy consumption and packet loss:\n",
        "\n",
        "1. **Track Min and Max Values**: Keep track of the minimum and maximum values for both energy consumption and packet loss.\n",
        "2. **Normalize Values**: Normalize the values of energy consumption and packet loss to a common scale (e.g., 0 to 1).\n",
        "\n",
        "### Updated Environment with Normalization\n",
        "\n",
        "Below is an updated version of the environment that includes normalization of energy consumption and packet loss:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class HERSoftwareEnvWithNormalizedGoals(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, config_space, energy_function, packet_loss_function, her_k=4):\n",
        "        super(HERSoftwareEnvWithNormalizedGoals, self).__init__()\n",
        "        \n",
        "        # Configuration space\n",
        "        self.config_space = config_space\n",
        "        self.action_space = spaces.MultiDiscrete([len(feature_space) for feature_space in config_space])\n",
        "        self.observation_space = spaces.MultiDiscrete([len(feature_space) for feature_space in config_space])\n",
        "        \n",
        "        # Performance functions\n",
        "        self.energy_function = energy_function\n",
        "        self.packet_loss_function = packet_loss_function\n",
        "        \n",
        "        # HER parameters\n",
        "        self.her_k = her_k\n",
        "        self.memory = deque(maxlen=10000)  # Replay buffer\n",
        "        \n",
        "        # Normalization parameters\n",
        "        self.energy_min = float('inf')\n",
        "        self.energy_max = float('-inf')\n",
        "        self.packet_loss_min = float('inf')\n",
        "        self.packet_loss_max = float('-inf')\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.state = self._get_random_config()\n",
        "        return self.state\n",
        "    \n",
        "    def _get_random_config(self):\n",
        "        return [np.random.choice(len(feature_space)) for feature_space in self.config_space]\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.state = action\n",
        "        energy_consumption = self.energy_function(self.state)\n",
        "        packet_loss = self.packet_loss_function(self.state)\n",
        "        \n",
        "        # Update min and max values for normalization\n",
        "        self.energy_min = min(self.energy_min, energy_consumption)\n",
        "        self.energy_max = max(self.energy_max, energy_consumption)\n",
        "        self.packet_loss_min = min(self.packet_loss_min, packet_loss)\n",
        "        self.packet_loss_max = max(self.packet_loss_max, packet_loss)\n",
        "        \n",
        "        reward = -self._normalize(energy_consumption, self.energy_min, self.energy_max)  # Primary goal: minimize normalized energy consumption\n",
        "        done = False\n",
        "        return self.state, reward, done, {'packet_loss': packet_loss}\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done, info):\n",
        "        self.memory.append((state, action, reward, next_state, done, info))\n",
        "    \n",
        "    def sample_memory(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def hindsight_replay(self):\n",
        "        her_batch = []\n",
        "        for state, action, reward, next_state, done, info in self.memory:\n",
        "            her_goals = self._sample_her_goals()\n",
        "            for goal in her_goals:\n",
        "                her_reward = self._compute_hindsight_reward(goal)\n",
        "                her_info = {'packet_loss': self.packet_loss_function(goal)}\n",
        "                her_batch.append((state, action, her_reward, goal, done, her_info))\n",
        "        self.memory.extend(her_batch)\n",
        "    \n",
        "    def _sample_her_goals(self):\n",
        "        # Sample k random future states as goals\n",
        "        future_states = [random.choice(self.memory)[3] for _ in range(self.her_k)]\n",
        "        return future_states\n",
        "    \n",
        "    def _compute_hindsight_reward(self, goal):\n",
        "        # Use the packet loss of the goal state as the hindsight reward\n",
        "        packet_loss = self.packet_loss_function(goal)\n",
        "        return -self._normalize(packet_loss, self.packet_loss_min, self.packet_loss_max)  # Hindsight goal: minimize normalized packet loss\n",
        "    \n",
        "    def _normalize(self, value, min_val, max_val):\n",
        "        # Normalize value to the range [0, 1]\n",
        "        if max_val > min_val:\n",
        "            return (value - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            return 0  # Avoid division by zero if all values are the same\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        energy = self.energy_function(self.state)\n",
        "        packet_loss = self.packet_loss_function(self.state)\n",
        "        norm_energy = self._normalize(energy, self.energy_min, self.energy_max)\n",
        "        norm_packet_loss = self._normalize(packet_loss, self.packet_loss_min, self.packet_loss_max)\n",
        "        print(f'Current Configuration: {self.state}, Energy Consumption: {energy} (normalized: {norm_energy}), Packet Loss: {packet_loss} (normalized: {norm_packet_loss})')\n",
        "\n",
        "# Example energy and packet loss functions\n",
        "def example_energy_function(config):\n",
        "    return sum(config)  # Dummy function: sum of the configuration vector\n",
        "\n",
        "def example_packet_loss_function(config):\n",
        "    return max(config)  # Dummy function: max of the configuration vector\n",
        "\n",
        "# Example configuration space: each feature has a range from 0 to 4\n",
        "config_space = [list(range(5)), list(range(5)), list(range(5))]\n",
        "\n",
        "# Create the environment\n",
        "env = HERSoftwareEnvWithNormalizedGoals(config_space, example_energy_function, example_packet_loss_function)\n",
        "```\n",
        "\n",
        "### Training the Agent with Normalized Objectives\n",
        "\n",
        "```python\n",
        "# Example usage with HER\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Replace with a policy action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        env.store_transition(state, action, reward, next_state, done, info)\n",
        "        state = next_state\n",
        "    \n",
        "    env.hindsight_replay()  # Augment the replay buffer with HER\n",
        "\n",
        "# Example of sampling from memory for training\n",
        "batch_size = 64\n",
        "samples = env.sample_memory(batch_size)\n",
        "```\n",
        "\n",
        "### Explanation of Key Changes\n",
        "\n",
        "1. **Normalization Function**:\n",
        "    - The `_normalize` function scales values to the [0, 1] range using observed minimum and maximum values.\n",
        "    \n",
        "2. **Tracking Min and Max Values**:\n",
        "    - During each step, the environment updates the minimum and maximum observed values for energy consumption and packet loss.\n",
        "\n",
        "3. **Normalized Rewards**:\n",
        "    - Rewards are computed based on normalized values, ensuring that energy consumption and packet loss are on a similar scale.\n",
        "    \n",
        "4. **Hindsight Replay with Normalized Goals**:\n",
        "    - In the hindsight replay, the reward is recomputed based on normalized packet loss, encouraging the agent to consider both objectives effectively.\n",
        "\n",
        "By normalizing the objectives, you ensure that the learning process is not biased towards one objective over the other, allowing the agent to explore the search space more effectively and find configurations that optimize both energy consumption and packet loss."
      ],
      "metadata": {
        "id": "VL5L3YzLdI-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you consider the state as consisting of energy consumption and packet loss, and actions as selecting from a set of configurations, the environment and agent interaction will change. The state representation will focus on performance metrics (energy consumption and packet loss) rather than the configurations themselves, and the agent's actions will involve selecting from predefined configurations.\n",
        "\n",
        "### Key Components\n",
        "- **State**: A vector consisting of energy consumption and packet loss.\n",
        "- **Action**: Index of a configuration vector from a predefined set of configurations.\n",
        "- **Reward**: A function of energy consumption and packet loss.\n",
        "\n",
        "### Steps to Implement\n",
        "\n",
        "#### 1. Define the Environment\n",
        "\n",
        "Define an environment where the state is a tuple of energy consumption and packet loss, and actions are indices to the configuration space.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class HERSoftwareEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, config_space, energy_function, packet_loss_function, her_k=4):\n",
        "        super(HERSoftwareEnv, self).__init__()\n",
        "        \n",
        "        # Configuration space\n",
        "        self.config_space = config_space\n",
        "        self.num_configs = len(config_space)\n",
        "        self.action_space = spaces.Discrete(self.num_configs)\n",
        "        \n",
        "        # State space (energy consumption, packet loss)\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(2,), dtype=np.float32)\n",
        "        \n",
        "        # Performance functions\n",
        "        self.energy_function = energy_function\n",
        "        self.packet_loss_function = packet_loss_function\n",
        "        \n",
        "        # HER parameters\n",
        "        self.her_k = her_k\n",
        "        self.memory = deque(maxlen=10000)  # Replay buffer\n",
        "        \n",
        "        # Normalization parameters\n",
        "        self.energy_min = float('inf')\n",
        "        self.energy_max = float('-inf')\n",
        "        self.packet_loss_min = float('inf')\n",
        "        self.packet_loss_max = float('-inf')\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.current_config = self._get_random_config()\n",
        "        energy_consumption = self.energy_function(self.current_config)\n",
        "        packet_loss = self.packet_loss_function(self.current_config)\n",
        "        self.state = np.array([energy_consumption, packet_loss])\n",
        "        return self.state\n",
        "    \n",
        "    def _get_random_config(self):\n",
        "        return random.choice(self.config_space)\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.current_config = self.config_space[action]\n",
        "        energy_consumption = self.energy_function(self.current_config)\n",
        "        packet_loss = self.packet_loss_function(self.current_config)\n",
        "        \n",
        "        # Update min and max values for normalization\n",
        "        self.energy_min = min(self.energy_min, energy_consumption)\n",
        "        self.energy_max = max(self.energy_max, energy_consumption)\n",
        "        self.packet_loss_min = min(self.packet_loss_min, packet_loss)\n",
        "        self.packet_loss_max = max(self.packet_loss_max, packet_loss)\n",
        "        \n",
        "        reward = -self._normalize(energy_consumption, self.energy_min, self.energy_max)  # Primary goal: minimize normalized energy consumption\n",
        "        self.state = np.array([energy_consumption, packet_loss])\n",
        "        done = False\n",
        "        return self.state, reward, done, {}\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample_memory(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def hindsight_replay(self):\n",
        "        her_batch = []\n",
        "        for state, action, reward, next_state, done in self.memory:\n",
        "            her_goals = self._sample_her_goals()\n",
        "            for goal in her_goals:\n",
        "                her_reward = self._compute_hindsight_reward(goal)\n",
        "                her_batch.append((state, action, her_reward, goal, done))\n",
        "        self.memory.extend(her_batch)\n",
        "    \n",
        "    def _sample_her_goals(self):\n",
        "        # Sample k random future states as goals\n",
        "        future_states = [random.choice(self.memory)[3] for _ in range(self.her_k)]\n",
        "        return future_states\n",
        "    \n",
        "    def _compute_hindsight_reward(self, goal):\n",
        "        # Use the packet loss of the goal state as the hindsight reward\n",
        "        packet_loss = goal[1]\n",
        "        return -self._normalize(packet_loss, self.packet_loss_min, self.packet_loss_max)  # Hindsight goal: minimize normalized packet loss\n",
        "    \n",
        "    def _normalize(self, value, min_val, max_val):\n",
        "        # Normalize value to the range [0, 1]\n",
        "        if max_val > min_val:\n",
        "            return (value - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            return 0  # Avoid division by zero if all values are the same\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        energy = self.state[0]\n",
        "        packet_loss = self.state[1]\n",
        "        norm_energy = self._normalize(energy, self.energy_min, self.energy_max)\n",
        "        norm_packet_loss = self._normalize(packet_loss, self.packet_loss_min, self.packet_loss_max)\n",
        "        print(f'Current State: [Energy Consumption: {energy} (normalized: {norm_energy}), Packet Loss: {packet_loss} (normalized: {norm_packet_loss})]')\n",
        "\n",
        "# Example energy and packet loss functions\n",
        "def example_energy_function(config):\n",
        "    return sum(config)  # Dummy function: sum of the configuration vector\n",
        "\n",
        "def example_packet_loss_function(config):\n",
        "    return max(config)  # Dummy function: max of the configuration vector\n",
        "\n",
        "# Example configuration space: list of configuration vectors\n",
        "config_space = [\n",
        "    [0, 1, 2],\n",
        "    [1, 1, 1],\n",
        "    [2, 0, 0],\n",
        "    [0, 0, 3],\n",
        "    [1, 2, 2],\n",
        "    [2, 1, 1]\n",
        "]\n",
        "\n",
        "# Create the environment\n",
        "env = HERSoftwareEnv(config_space, example_energy_function, example_packet_loss_function)\n",
        "```\n",
        "\n",
        "### Training the Agent with Normalized Objectives\n",
        "\n",
        "```python\n",
        "# Example usage with HER\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Replace with a policy action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        env.store_transition(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "    \n",
        "    env.hindsight_replay()  # Augment the replay buffer with HER\n",
        "\n",
        "# Example of sampling from memory for training\n",
        "batch_size = 64\n",
        "samples = env.sample_memory(batch_size)\n",
        "```\n",
        "\n",
        "### Explanation of Key Changes\n",
        "\n",
        "1. **State Representation**:\n",
        "    - The state is represented as a tuple of energy consumption and packet loss.\n",
        "\n",
        "2. **Action Space**:\n",
        "    - The actions are discrete and represent indices of configurations in the configuration space.\n",
        "\n",
        "3. **Reward Function**:\n",
        "    - The reward is based on normalized energy consumption, but the hindsight reward is based on normalized packet loss.\n",
        "\n",
        "4. **Normalization**:\n",
        "    - Both energy consumption and packet loss are normalized based on their observed minimum and maximum values.\n",
        "\n",
        "5. **Hindsight Replay**:\n",
        "    - During hindsight replay, the agent recomputes the reward based on packet loss, encouraging exploration of configurations that might be optimal for packet loss as well.\n",
        "\n",
        "By structuring the problem this way, the agent learns to balance energy consumption and packet loss, using HER to explore the search space more effectively. Normalizing the objectives ensures that both performance metrics are considered equally during the learning process."
      ],
      "metadata": {
        "id": "pt348Q7_e7wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the best state representation depends on various factors, including the complexity of the problem, the availability of information, and the specific goals you want to achieve. Here’s a comparative analysis of both state representations:\n",
        "\n",
        "### State Representation 1: Configuration Vector as State\n",
        "- **State**: The configuration vector itself.\n",
        "- **Action**: Each element of the configuration vector (multi-dimensional action space).\n",
        "- **Reward**: Based on performance metrics (e.g., energy consumption and packet loss).\n",
        "\n",
        "#### Advantages:\n",
        "1. **Direct Representation**: The agent directly interacts with the actual configuration, making it easier to understand the impact of each configuration on performance.\n",
        "2. **Exploratory Power**: The agent can explore the configuration space more freely, potentially discovering novel configurations.\n",
        "\n",
        "#### Disadvantages:\n",
        "1. **High Dimensionality**: The action space can become large, especially if the configuration vector has many elements, making learning more complex.\n",
        "2. **Sparse Rewards**: The agent may receive sparse and delayed rewards, especially in high-dimensional spaces, which can make training slower and less stable.\n",
        "\n",
        "### State Representation 2: Performance Metrics as State\n",
        "- **State**: Energy consumption and packet loss (performance metrics).\n",
        "- **Action**: Selecting a configuration from a predefined set (discrete action space).\n",
        "- **Reward**: Based on normalized performance metrics.\n",
        "\n",
        "#### Advantages:\n",
        "1. **Simplified Action Space**: The action space is simpler (discrete), which can make the learning process more efficient.\n",
        "2. **Focused Learning**: The agent focuses directly on performance outcomes, potentially leading to faster convergence.\n",
        "\n",
        "#### Disadvantages:\n",
        "1. **Indirect Representation**: The agent doesn't interact directly with the configuration parameters, which might limit its ability to understand the underlying system dynamics.\n",
        "2. **Dependence on Predefined Configurations**: The quality of learning depends heavily on the diversity and comprehensiveness of the predefined configuration set.\n",
        "\n",
        "### Which is Better?\n",
        "\n",
        "- **For Complex Configuration Spaces**: If the configuration space is large and complex, and you want the agent to learn intricate relationships between different configuration parameters, the first representation (configuration vector as state) might be better.\n",
        "- **For Focused Optimization**: If the primary goal is to optimize specific performance metrics efficiently and the configuration space can be represented well by a finite set of configurations, the second representation (performance metrics as state) might be more effective.\n",
        "\n",
        "### Recommended Approach\n",
        "\n",
        "Given that your primary goal is to minimize energy consumption with packet loss as a hindsight goal, the second representation (performance metrics as state) might offer a more straightforward and efficient way to achieve this. It simplifies the action space and directly ties state changes to performance outcomes.\n",
        "\n",
        "### Implementing the Performance Metrics as State\n",
        "\n",
        "Below is the adjusted code snippet for the second approach with performance metrics as the state:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class HERSoftwareEnvWithNormalizedGoals(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, config_space, energy_function, packet_loss_function, her_k=4):\n",
        "        super(HERSoftwareEnvWithNormalizedGoals, self).__init__()\n",
        "        \n",
        "        # Configuration space\n",
        "        self.config_space = config_space\n",
        "        self.num_configs = len(config_space)\n",
        "        self.action_space = spaces.Discrete(self.num_configs)\n",
        "        \n",
        "        # State space (energy consumption, packet loss)\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(2,), dtype=np.float32)\n",
        "        \n",
        "        # Performance functions\n",
        "        self.energy_function = energy_function\n",
        "        self.packet_loss_function = packet_loss_function\n",
        "        \n",
        "        # HER parameters\n",
        "        self.her_k = her_k\n",
        "        self.memory = deque(maxlen=10000)  # Replay buffer\n",
        "        \n",
        "        # Normalization parameters\n",
        "        self.energy_min = float('inf')\n",
        "        self.energy_max = float('-inf')\n",
        "        self.packet_loss_min = float('inf')\n",
        "        self.packet_loss_max = float('-inf')\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.current_config = self._get_random_config()\n",
        "        energy_consumption = self.energy_function(self.current_config)\n",
        "        packet_loss = self.packet_loss_function(self.current_config)\n",
        "        self.state = np.array([energy_consumption, packet_loss])\n",
        "        return self.state\n",
        "    \n",
        "    def _get_random_config(self):\n",
        "        return random.choice(self.config_space)\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.current_config = self.config_space[action]\n",
        "        energy_consumption = self.energy_function(self.current_config)\n",
        "        packet_loss = self.packet_loss_function(self.current_config)\n",
        "        \n",
        "        # Update min and max values for normalization\n",
        "        self.energy_min = min(self.energy_min, energy_consumption)\n",
        "        self.energy_max = max(self.energy_max, energy_consumption)\n",
        "        self.packet_loss_min = min(self.packet_loss_min, packet_loss)\n",
        "        self.packet_loss_max = max(self.packet_loss_max, packet_loss)\n",
        "        \n",
        "        reward = -self._normalize(energy_consumption, self.energy_min, self.energy_max)  # Primary goal: minimize normalized energy consumption\n",
        "        self.state = np.array([energy_consumption, packet_loss])\n",
        "        done = False\n",
        "        return self.state, reward, done, {}\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample_memory(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def hindsight_replay(self):\n",
        "        her_batch = []\n",
        "        for state, action, reward, next_state, done in self.memory:\n",
        "            her_goals = self._sample_her_goals()\n",
        "            for goal in her_goals:\n",
        "                her_reward = self._compute_hindsight_reward(goal)\n",
        "                her_batch.append((state, action, her_reward, goal, done))\n",
        "        self.memory.extend(her_batch)\n",
        "    \n",
        "    def _sample_her_goals(self):\n",
        "        # Sample k random future states as goals\n",
        "        future_states = [random.choice(self.memory)[3] for _ in range(self.her_k)]\n",
        "        return future_states\n",
        "    \n",
        "    def _compute_hindsight_reward(self, goal):\n",
        "        # Use the packet loss of the goal state as the hindsight reward\n",
        "        packet_loss = goal[1]\n",
        "        return -self._normalize(packet_loss, self.packet_loss_min, self.packet_loss_max)  # Hindsight goal: minimize normalized packet loss\n",
        "    \n",
        "    def _normalize(self, value, min_val, max_val):\n",
        "        # Normalize value to the range [0, 1]\n",
        "        if max_val > min_val:\n",
        "            return (value - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            return 0  # Avoid division by zero if all values are the same\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        energy = self.state[0]\n",
        "        packet_loss = self.state[1]\n",
        "        norm_energy = self._normalize(energy, self.energy_min, self.energy_max)\n",
        "        norm_packet_loss = self._normalize(packet_loss, self.packet_loss_min, self.packet_loss_max)\n",
        "        print(f'Current State: [Energy Consumption: {energy} (normalized: {norm_energy}), Packet Loss: {packet_loss} (normalized: {norm_packet_loss})]')\n",
        "\n",
        "# Example energy and packet loss functions\n",
        "def example_energy_function(config):\n",
        "    return sum(config)  # Dummy function: sum of the configuration vector\n",
        "\n",
        "def example_packet_loss_function(config):\n",
        "    return max(config)  # Dummy function: max of the configuration vector\n",
        "\n",
        "# Example configuration space: list of configuration vectors\n",
        "config_space = [\n",
        "    [0, 1, 2],\n",
        "    [1, 1, 1],\n",
        "    [2, 0, 0],\n",
        "    [0, 0, 3],\n",
        "    [1, 2, 2],\n",
        "    [2, 1, 1]\n",
        "]\n",
        "\n",
        "# Create the environment\n",
        "env = HERSoftwareEnvWithNormalizedGoals(config_space, example_energy_function, example_packet_loss_function)\n",
        "```\n",
        "\n",
        "### Training the Agent with Normalized Objectives\n",
        "\n",
        "```python\n",
        "# Example usage with HER\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Replace with a policy action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        env.store_transition(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "    \n",
        "    env.hindsight_replay()  # Augment the replay buffer with HER\n",
        "\n",
        "# Example of sampling from memory for training\n",
        "batch_size = 64\n",
        "samples = env.sample_memory(batch_size)\n",
        "```\n",
        "\n",
        "In this setup, the agent learns to balance between minimizing energy consumption and packet loss, taking advantage of the simpler state and action representations, and effectively using HER to improve exploration and learning efficiency."
      ],
      "metadata": {
        "id": "RELlewsMiOsC"
      }
    }
  ]
}