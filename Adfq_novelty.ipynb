{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfSuxFZNjMDgyUz8h4Do/Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezakavianifar/RL-DeltaIoT/blob/main/Adfq_novelty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improving the `adjust_penalty` function can be approached from several angles, focusing on enhancing its flexibility, robustness, and effectiveness in guiding the learning process. Here are some suggestions:\n",
        "\n",
        "### 1. **Dynamic Learning Rate**\n",
        "- **Description**: Use an adaptive learning rate instead of a fixed one to adjust the penalty. This can help the algorithm to respond more dynamically to the changes in the environment.\n",
        "- **Implementation**: Incorporate a learning rate scheduler that decreases the learning rate over time or based on the number of iterations.\n",
        "\n",
        "### 2. **Incorporate More State Information**\n",
        "- **Description**: Consider additional metrics from the states, such as other similarity measures or features, to make the penalty adjustment more informed.\n",
        "- **Implementation**: Extend the function to account for other state-related metrics or features that might provide a more comprehensive picture of state transitions.\n",
        "\n",
        "### 3. **Non-Linear Adjustment**\n",
        "- **Description**: Use non-linear functions (e.g., sigmoid, tanh) for penalty adjustment to provide a smoother transition and avoid abrupt changes.\n",
        "- **Implementation**: Apply a non-linear function to the penalty adjustment calculation.\n",
        "\n",
        "### 4. **Historical Reward Trends**\n",
        "- **Description**: Instead of only considering the previous reward, take into account a moving average or a weighted sum of past rewards to smooth out fluctuations.\n",
        "- **Implementation**: Maintain a running average of rewards and use this averaged value in the penalty adjustment.\n",
        "\n",
        "### 5. **Different Penalty Ranges**\n",
        "- **Description**: Allow the penalty to have different ranges or different limits based on the context or specific needs of the task.\n",
        "- **Implementation**: Adjust the penalty bounds dynamically based on performance metrics or state characteristics.\n",
        "\n",
        "### 6. **Normalization**\n",
        "- **Description**: Normalize the rewards and similarities before using them in the adjustment calculations to ensure they are on a comparable scale.\n",
        "- **Implementation**: Implement normalization functions to scale rewards and similarities.\n",
        "\n",
        "### 7. **More Robust Initialization**\n",
        "- **Description**: Ensure that the initial values of `self.prev_state`, `self.prev_cosine_similarity`, and `self.prev_reward` are set thoughtfully to avoid instability at the start.\n",
        "- **Implementation**: Set initial values based on the first observed state or a predefined starting condition.\n",
        "\n",
        "### 8. **Logging and Analysis**\n",
        "- **Description**: Incorporate logging and detailed analysis of the penalty adjustments to monitor how the penalty evolves over time.\n",
        "- **Implementation**: Add logging statements and periodically analyze logs to fine-tune the adjustment process.\n",
        "\n",
        "Hereâ€™s an enhanced version of the function incorporating some of these suggestions:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "class DRLAlgorithm:\n",
        "    def __init__(self, learning_rate=0.01, penalty_bounds=(0.0, 1.0)):\n",
        "        self.c_penalty = 0.5\n",
        "        self.learning_rate = learning_rate\n",
        "        self.penalty_bounds = penalty_bounds\n",
        "        self.prev_state = None\n",
        "        self.prev_cosine_similarity = None\n",
        "        self.prev_reward = None\n",
        "        self.reward_moving_avg = 0.0\n",
        "        self.moving_avg_alpha = 0.1  # Weight for moving average\n",
        "\n",
        "    def adjust_penalty(self, reward, cosine_similarity):\n",
        "        if self.prev_state is not None:\n",
        "            # Calculate moving average of rewards\n",
        "            self.reward_moving_avg = (\n",
        "                self.moving_avg_alpha * reward + (1 - self.moving_avg_alpha) * self.reward_moving_avg\n",
        "            )\n",
        "\n",
        "            similarity_change = cosine_similarity - self.prev_cosine_similarity\n",
        "            reward_change = reward - self.reward_moving_avg\n",
        "\n",
        "            if reward_change > 0:\n",
        "                if similarity_change >= 0:\n",
        "                    self.c_penalty -= self.learning_rate * similarity_change * reward_change\n",
        "                else:\n",
        "                    self.c_penalty += self.learning_rate * similarity_change * reward_change\n",
        "            else:\n",
        "                self.c_penalty += self.learning_rate * similarity_change * reward_change\n",
        "\n",
        "            # Apply a sigmoid function to smooth the penalty adjustments\n",
        "            self.c_penalty = 1 / (1 + np.exp(-self.c_penalty))\n",
        "\n",
        "            # Ensure penalty stays within bounds\n",
        "            self.c_penalty = max(self.penalty_bounds[0], min(self.penalty_bounds[1], self.c_penalty))\n",
        "\n",
        "        # Update previous values for next iteration\n",
        "        self.prev_reward = reward\n",
        "        self.prev_cosine_similarity = cosine_similarity\n",
        "```\n",
        "\n",
        "### Explanation of Changes:\n",
        "1. **Moving Average of Rewards**: A moving average of rewards is used to smooth out reward fluctuations.\n",
        "2. **Sigmoid Function**: Applied to `self.c_penalty` for non-linear adjustments.\n",
        "3. **Bounded Penalty**: Ensured penalty stays within dynamically defined bounds.\n",
        "4. **Initialization**: Included initialization for the moving average and other relevant parameters.\n",
        "\n",
        "These improvements aim to make the penalty adjustment more adaptive, robust, and better aligned with the overall learning process."
      ],
      "metadata": {
        "id": "G_8Ujrq1jHR1"
      }
    }
  ]
}