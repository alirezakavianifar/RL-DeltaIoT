{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjczwM46GKaBOqw4rHMne0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezakavianifar/RL-DeltaIoT/blob/main/novelIdeas2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwEBmIneGy-A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To illustrate how an auxiliary objective can help a main objective, let's refer to the concept of Meta Multi-Objectivization (MMO) for software configuration tuning from the provided document. The core idea of MMO is to incorporate an auxiliary performance objective to aid in optimizing a primary performance objective, thereby preventing the search process from being trapped in local optima.\n",
        "\n",
        "Here's a summary of how MMO uses an auxiliary objective to enhance the main objective:\n",
        "\n",
        "### Main Concept\n",
        "\n",
        "- **Main Objective**: The primary performance metric we aim to optimize (e.g., minimizing latency).\n",
        "- **Auxiliary Objective**: A secondary performance metric not directly optimized but used to influence the search process (e.g., throughput).\n",
        "\n",
        "### How It Works\n",
        "\n",
        "1. **Initial Problem**: Optimizing a single performance attribute can be challenging due to the complex configuration landscape and the risk of being trapped in local optima.\n",
        "\n",
        "2. **Introducing Auxiliary Objective**: Instead of focusing solely on the main objective, MMO introduces an auxiliary performance objective. This objective is not optimized directly but used to make configurations less comparable, thereby facilitating a more diverse search.\n",
        "\n",
        "3. **Pareto Nondomination**: By considering both objectives, configurations that perform similarly on the main objective but differ on the auxiliary objective become Pareto nondominated to each other. This reduces the likelihood of the search getting stuck in local optima.\n",
        "\n",
        "4. **Normalization Method**: A new normalization method is designed to effectively use the MMO model without the sensitivity to weights, ensuring balanced optimization.\n",
        "\n",
        "### Example\n",
        "\n",
        "Consider the example of tuning the configurations of Apache Storm for minimizing latency:\n",
        "\n",
        "- **Main Objective**: Minimize latency.\n",
        "- **Auxiliary Objective**: Throughput.\n",
        "\n",
        "In a traditional single-objective optimization, configurations are evaluated based solely on latency. This can lead to the optimizer being trapped in configurations that seem optimal locally but are not globally optimal.\n",
        "\n",
        "In MMO, both latency and throughput are considered. The optimizer evaluates configurations based on their performance on both metrics, making configurations with different throughputs but similar latencies less comparable. This broader perspective allows the optimizer to explore a wider configuration space, improving the chances of escaping local optima and finding a better global solution.\n",
        "\n",
        "### Benefits\n",
        "\n",
        "- **Diverse Search Space**: By considering the auxiliary objective, the optimizer explores a more diverse set of configurations.\n",
        "- **Reduced Local Optima Traps**: Pareto nondomination helps in avoiding local optima by comparing configurations on multiple fronts.\n",
        "- **Resource Efficiency**: The new normalization method reduces the sensitivity to weight parameters, leading to more efficient use of resources.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Using an auxiliary objective in MMO helps in enhancing the optimization of the main objective by preventing local optima traps and promoting a more diverse and effective search process.\n",
        "\n",
        "**Related Query Suggestions:**\n",
        "**a.** Can you explain the normalization method used in MMO in more detail?\n",
        "**b.** How does the new normalization method improve the efficiency of MMO compared to the traditional normalization method?"
      ],
      "metadata": {
        "id": "SH5H_OueG0ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explain how the Meta Multi-Objectivization (MMO) method works using numbers, let's walk through a simplified example.\n",
        "\n",
        "### Scenario:\n",
        "Imagine we are tuning configurations for Apache Storm with the following objectives:\n",
        "- **Main Objective (Latency)**: Minimize latency (ms).\n",
        "- **Auxiliary Objective (Throughput)**: Maximize throughput (msgs/ms).\n",
        "\n",
        "### Initial Setup:\n",
        "We have four configurations (A, B, C, D) with their respective latency and throughput values.\n",
        "\n",
        "| Configuration | Latency (ms) | Throughput (msgs/ms) |\n",
        "|---------------|--------------|----------------------|\n",
        "| A             | 100          | 3.0                  |\n",
        "| B             | 120          | 2.5                  |\n",
        "| C             | 110          | 4.0                  |\n",
        "| D             | 130          | 3.5                  |\n",
        "\n",
        "### Step-by-Step Explanation:\n",
        "\n",
        "1. **Normalization**:\n",
        "   - First, we normalize the latency and throughput values to a common scale. We'll use min-max normalization based on the given configurations.\n",
        "\n",
        "   For Latency:\n",
        "   - Min = 100, Max = 130\n",
        "\n",
        "   For Throughput:\n",
        "   - Min = 2.5, Max = 4.0\n",
        "\n",
        "   Normalized values are calculated as follows:\n",
        "   \\[\n",
        "   \\text{Normalized Value} = \\frac{\\text{Value} - \\text{Min}}{\\text{Max} - \\text{Min}}\n",
        "   \\]\n",
        "\n",
        "   Applying this formula:\n",
        "\n",
        "   | Configuration | Latency (Normalized) | Throughput (Normalized) |\n",
        "   |---------------|-----------------------|-------------------------|\n",
        "   | A             | \\(\\frac{100-100}{130-100} = 0.0\\)          | \\(\\frac{3.0-2.5}{4.0-2.5} = 0.333\\)        |\n",
        "   | B             | \\(\\frac{120-100}{130-100} = 0.667\\)        | \\(\\frac{2.5-2.5}{4.0-2.5} = 0.0\\)          |\n",
        "   | C             | \\(\\frac{110-100}{130-100} = 0.333\\)        | \\(\\frac{4.0-2.5}{4.0-2.5} = 1.0\\)          |\n",
        "   | D             | \\(\\frac{130-100}{130-100} = 1.0\\)          | \\(\\frac{3.5-2.5}{4.0-2.5} = 0.667\\)        |\n",
        "\n",
        "2. **Form Meta-Objectives**:\n",
        "   - Using MMO, we form two meta-objectives \\(g1\\) and \\(g2\\) by combining the normalized values of latency (\\(f_t\\)) and throughput (\\(f_a\\)):\n",
        "\n",
        "   \\[\n",
        "   g1(x) = f_t(x) + f_a(x)\n",
        "   \\]\n",
        "   \\[\n",
        "   g2(x) = f_t(x) - f_a(x)\n",
        "   \\]\n",
        "\n",
        "   Calculate \\(g1\\) and \\(g2\\) for each configuration:\n",
        "\n",
        "   | Configuration | \\(g1 = f_t + f_a\\)   | \\(g2 = f_t - f_a\\)   |\n",
        "   |---------------|-----------------------|-----------------------|\n",
        "   | A             | \\(0.0 + 0.333 = 0.333\\) | \\(0.0 - 0.333 = -0.333\\) |\n",
        "   | B             | \\(0.667 + 0.0 = 0.667\\) | \\(0.667 - 0.0 = 0.667\\)  |\n",
        "   | C             | \\(0.333 + 1.0 = 1.333\\) | \\(0.333 - 1.0 = -0.667\\) |\n",
        "   | D             | \\(1.0 + 0.667 = 1.667\\) | \\(1.0 - 0.667 = 0.333\\)  |\n",
        "\n",
        "3. **Determine Pareto Dominance**:\n",
        "   - Identify Pareto optimal configurations based on \\(g1\\) and \\(g2\\).\n",
        "   - A configuration \\(X\\) is Pareto dominated by \\(Y\\) if \\(Y\\) is at least as good in all objectives and strictly better in at least one.\n",
        "\n",
        "   By comparing configurations:\n",
        "\n",
        "   - **A** is dominated by **B**, **C**, and **D** because \\(A\\)'s \\(g1\\) and \\(g2\\) values are lower.\n",
        "   - **B** is not dominated by any other configuration because \\(B\\)'s \\(g2\\) is the lowest positive value.\n",
        "   - **C** has the highest \\(g1\\) but is dominated by **D** due to \\(D\\)'s higher \\(g1\\) and lower \\(g2\\).\n",
        "   - **D** is not dominated by any other configuration due to the highest \\(g1\\) and intermediate \\(g2\\) value.\n",
        "\n",
        "4. **Final Selection**:\n",
        "   - Configurations that are not dominated form the Pareto front. In this case, **B** and **D** are on the Pareto front as they are not dominated by any other configuration.\n",
        "\n",
        "### Conclusion:\n",
        "By considering both latency and throughput in MMO, the search process evaluates configurations on a combined basis rather than just one objective. This helps prevent the optimizer from getting stuck in local optima by considering a broader perspective of performance, ensuring more robust and effective optimization.\n",
        "\n",
        "This numerical example demonstrates how the MMO method works, making configurations with similar performance on the main objective but differing on the auxiliary objective less comparable, thus enhancing the search process."
      ],
      "metadata": {
        "id": "57GR_StAG2Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an IoT network where the primary goal is to minimize energy consumption and the auxiliary objective is packet loss, the Meta Multi-Objectivization (MMO) method can help improve optimization by considering both objectives simultaneously. Let's go through a numerical example to illustrate how this works.\n",
        "\n",
        "### Scenario:\n",
        "- **Main Objective (Energy Consumption)**: Minimize energy consumption (Joules).\n",
        "- **Auxiliary Objective (Packet Loss)**: Minimize packet loss (percentage).\n",
        "\n",
        "### Initial Setup:\n",
        "Assume we have four configurations (A, B, C, D) with their respective energy consumption and packet loss values.\n",
        "\n",
        "| Configuration | Energy Consumption (J) | Packet Loss (%) |\n",
        "|---------------|-------------------------|-----------------|\n",
        "| A             | 10                      | 5               |\n",
        "| B             | 15                      | 3               |\n",
        "| C             | 12                      | 8               |\n",
        "| D             | 20                      | 2               |\n",
        "\n",
        "### Step-by-Step Explanation:\n",
        "\n",
        "1. **Normalization**:\n",
        "   - First, we normalize the energy consumption and packet loss values to a common scale using min-max normalization.\n",
        "\n",
        "   For Energy Consumption:\n",
        "   - Min = 10, Max = 20\n",
        "\n",
        "   For Packet Loss:\n",
        "   - Min = 2, Max = 8\n",
        "\n",
        "   Normalized values are calculated as follows:\n",
        "   \\[\n",
        "   \\text{Normalized Value} = \\frac{\\text{Value} - \\text{Min}}{\\text{Max} - \\text{Min}}\n",
        "   \\]\n",
        "\n",
        "   Applying this formula:\n",
        "\n",
        "   | Configuration | Energy Consumption (Normalized) | Packet Loss (Normalized) |\n",
        "   |---------------|----------------------------------|---------------------------|\n",
        "   | A             | \\(\\frac{10-10}{20-10} = 0.0\\)              | \\(\\frac{5-2}{8-2} = 0.5\\)                |\n",
        "   | B             | \\(\\frac{15-10}{20-10} = 0.5\\)              | \\(\\frac{3-2}{8-2} = 0.167\\)              |\n",
        "   | C             | \\(\\frac{12-10}{20-10} = 0.2\\)              | \\(\\frac{8-2}{8-2} = 1.0\\)                |\n",
        "   | D             | \\(\\frac{20-10}{20-10} = 1.0\\)              | \\(\\frac{2-2}{8-2} = 0.0\\)                |\n",
        "\n",
        "2. **Form Meta-Objectives**:\n",
        "   - Using MMO, we form two meta-objectives \\(g1\\) and \\(g2\\) by combining the normalized values of energy consumption (\\(f_t\\)) and packet loss (\\(f_a\\)):\n",
        "\n",
        "   \\[\n",
        "   g1(x) = f_t(x) + f_a(x)\n",
        "   \\]\n",
        "   \\[\n",
        "   g2(x) = f_t(x) - f_a(x)\n",
        "   \\]\n",
        "\n",
        "   Calculate \\(g1\\) and \\(g2\\) for each configuration:\n",
        "\n",
        "   | Configuration | \\(g1 = f_t + f_a\\)   | \\(g2 = f_t - f_a\\)   |\n",
        "   |---------------|----------------------|----------------------|\n",
        "   | A             | \\(0.0 + 0.5 = 0.5\\)  | \\(0.0 - 0.5 = -0.5\\) |\n",
        "   | B             | \\(0.5 + 0.167 = 0.667\\) | \\(0.5 - 0.167 = 0.333\\) |\n",
        "   | C             | \\(0.2 + 1.0 = 1.2\\)  | \\(0.2 - 1.0 = -0.8\\) |\n",
        "   | D             | \\(1.0 + 0.0 = 1.0\\)  | \\(1.0 - 0.0 = 1.0\\)  |\n",
        "\n",
        "3. **Determine Pareto Dominance**:\n",
        "   - Identify Pareto optimal configurations based on \\(g1\\) and \\(g2\\).\n",
        "   - A configuration \\(X\\) is Pareto dominated by \\(Y\\) if \\(Y\\) is at least as good in all objectives and strictly better in at least one.\n",
        "\n",
        "   By comparing configurations:\n",
        "\n",
        "   - **A** is not dominated by any other configuration because \\(A\\)'s \\(g2\\) is the lowest negative value.\n",
        "   - **B** is not dominated by any other configuration because \\(B\\)'s \\(g1\\) and \\(g2\\) values are moderate.\n",
        "   - **C** is dominated by **A** and **B** due to \\(C\\)'s higher \\(g1\\) and lower \\(g2\\) values.\n",
        "   - **D** is not dominated by any other configuration because \\(D\\)'s \\(g1\\) is the highest positive value.\n",
        "\n",
        "4. **Final Selection**:\n",
        "   - Configurations that are not dominated form the Pareto front. In this case, **A**, **B**, and **D** are on the Pareto front as they are not dominated by any other configuration.\n",
        "\n",
        "### Conclusion:\n",
        "By considering both energy consumption and packet loss in MMO, the search process evaluates configurations on a combined basis rather than just one objective. This helps prevent the optimizer from getting stuck in local optima by considering a broader perspective of performance, ensuring more robust and effective optimization.\n",
        "\n",
        "This numerical example demonstrates how the MMO method works, making configurations with similar performance on the main objective but differing on the auxiliary objective less comparable, thus enhancing the search process.\n",
        "\n",
        "**Related Query Suggestions:**\n",
        "**a.** Can you explain how Pareto dominance helps in improving the optimization process?\n",
        "**b.** How can we extend this approach to include more than two objectives?"
      ],
      "metadata": {
        "id": "ii5iRL1cG4Wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To integrate the principles of Meta Multi-Objectivization (MMO) into deep reinforcement learning (DRL) for an IoT network with the primary goal of minimizing energy consumption, we can design a multi-objective reinforcement learning (MORL) approach. Here's how you can do it:\n",
        "\n",
        "### Step-by-Step Approach:\n",
        "\n",
        "1. **Define Objectives**:\n",
        "   - **Main Objective**: Minimize energy consumption.\n",
        "   - **Auxiliary Objective**: Minimize packet loss (or any other relevant metric such as latency or throughput).\n",
        "\n",
        "2. **Reward Shaping**:\n",
        "   - Construct a composite reward function that considers both energy consumption and packet loss.\n",
        "   - Use weights to balance these objectives, similar to how weights are used in MMO to form meta-objectives.\n",
        "\n",
        "3. **Normalization**:\n",
        "   - Normalize both energy consumption and packet loss to a common scale to ensure they are comparable.\n",
        "   - Use min-max normalization or a similar technique to keep the values between 0 and 1.\n",
        "\n",
        "4. **Pareto Front Generation**:\n",
        "   - During training, maintain a Pareto front of solutions (policies) that are non-dominated with respect to the defined objectives.\n",
        "   - Use these solutions to guide the training process towards optimal policies.\n",
        "\n",
        "5. **Training with Multi-Objective Deep Reinforcement Learning**:\n",
        "   - Use a DRL algorithm (e.g., DDPG, PPO) that can handle multiple objectives.\n",
        "   - Modify the training algorithm to consider both objectives by maintaining separate value functions for each objective and combining them into a composite reward.\n",
        "\n",
        "### Example Implementation:\n",
        "\n",
        "Here is a simplified pseudocode and example implementation using a modified DDPG algorithm:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class MultiObjectiveDDPG:\n",
        "    def __init__(self, state_dim, action_dim, action_bound):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_bound = action_bound\n",
        "        \n",
        "        # Main and auxiliary objective weights\n",
        "        self.w1 = 0.5\n",
        "        self.w2 = 0.5\n",
        "        \n",
        "        # Actor and Critic Networks\n",
        "        self.actor = self.build_actor()\n",
        "        self.critic = self.build_critic()\n",
        "        \n",
        "        # Target Networks\n",
        "        self.target_actor = self.build_actor()\n",
        "        self.target_critic = self.build_critic()\n",
        "        \n",
        "        # Optimizers\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "        \n",
        "        # Replay Buffer\n",
        "        self.buffer = []\n",
        "        self.buffer_size = 100000\n",
        "        self.batch_size = 64\n",
        "    \n",
        "    def build_actor(self):\n",
        "        state_input = layers.Input(shape=(self.state_dim,))\n",
        "        h = layers.Dense(256, activation='relu')(state_input)\n",
        "        h = layers.Dense(256, activation='relu')(h)\n",
        "        output = layers.Dense(self.action_dim, activation='tanh')(h)\n",
        "        scaled_output = layers.Lambda(lambda x: x * self.action_bound)(output)\n",
        "        return tf.keras.Model(state_input, scaled_output)\n",
        "    \n",
        "    def build_critic(self):\n",
        "        state_input = layers.Input(shape=(self.state_dim,))\n",
        "        action_input = layers.Input(shape=(self.action_dim,))\n",
        "        concat = layers.Concatenate()([state_input, action_input])\n",
        "        h = layers.Dense(256, activation='relu')(concat)\n",
        "        h = layers.Dense(256, activation='relu')(h)\n",
        "        output = layers.Dense(1)(h)\n",
        "        return tf.keras.Model([state_input, action_input], output)\n",
        "    \n",
        "    def update_target(self, tau=0.005):\n",
        "        # Update target networks\n",
        "        for target_param, param in zip(self.target_actor.trainable_variables, self.actor.trainable_variables):\n",
        "            target_param.assign(tau * param + (1 - tau) * target_param)\n",
        "        \n",
        "        for target_param, param in zip(self.target_critic.trainable_variables, self.critic.trainable_variables):\n",
        "            target_param.assign(tau * param + (1 - tau) * target_param)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        return self.actor.predict(state)[0]\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) > self.buffer_size:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def train(self):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        batch = np.random.choice(len(self.buffer), self.batch_size)\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*[self.buffer[i] for i in batch])\n",
        "        \n",
        "        state_batch = np.array(state_batch)\n",
        "        action_batch = np.array(action_batch)\n",
        "        reward_batch = np.array(reward_batch)\n",
        "        next_state_batch = np.array(next_state_batch)\n",
        "        done_batch = np.array(done_batch)\n",
        "        \n",
        "        # Critic update\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor(next_state_batch)\n",
        "            y = reward_batch + (1 - done_batch) * 0.99 * self.target_critic([next_state_batch, target_actions])\n",
        "            critic_value = self.critic([state_batch, action_batch])\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "        \n",
        "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
        "        \n",
        "        # Actor update\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor(state_batch)\n",
        "            critic_value = self.critic([state_batch, actions])\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "        \n",
        "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
        "        \n",
        "        # Update target networks\n",
        "        self.update_target()\n",
        "    \n",
        "    def composite_reward(self, energy_consumption, packet_loss):\n",
        "        normalized_energy = (energy_consumption - self.min_energy) / (self.max_energy - self.min_energy)\n",
        "        normalized_packet_loss = (packet_loss - self.min_packet_loss) / (self.max_packet_loss - self.min_packet_loss)\n",
        "        return self.w1 * normalized_energy + self.w2 * normalized_packet_loss\n",
        "\n",
        "# Example usage\n",
        "state_dim = 10\n",
        "action_dim = 2\n",
        "action_bound = 1.0\n",
        "agent = MultiObjectiveDDPG(state_dim, action_dim, action_bound)\n",
        "\n",
        "# Training loop (simplified)\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, energy_consumption, packet_loss, done = env.step(action)\n",
        "        reward = agent.composite_reward(energy_consumption, packet_loss)\n",
        "        agent.store_transition(state, action, reward, next_state, done)\n",
        "        agent.train()\n",
        "        state = next_state\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "1. **Define Objectives**:\n",
        "   - Main Objective: Energy consumption is considered the primary objective.\n",
        "   - Auxiliary Objective: Packet loss is considered the secondary objective.\n",
        "\n",
        "2. **Reward Shaping**:\n",
        "   - `composite_reward` function is defined to combine both objectives into a single reward signal using weights.\n",
        "\n",
        "3. **Normalization**:\n",
        "   - Both energy consumption and packet loss are normalized to ensure comparability.\n",
        "\n",
        "4. **Pareto Front Generation**:\n",
        "   - Not explicitly shown in this example but can be maintained by tracking policies that are non-dominated based on their performance across episodes.\n",
        "\n",
        "5. **Training**:\n",
        "   - The DDPG algorithm is modified to use the composite reward, balancing between energy consumption and packet loss.\n",
        "\n",
        "### Conclusion:\n",
        "By integrating the principles of MMO into DRL, you can effectively optimize an IoT network for energy consumption while considering packet loss. This approach ensures that the optimization process does not get stuck in local optima and explores a diverse set of configurations, leading to more robust and efficient policies."
      ],
      "metadata": {
        "id": "hA9Yvun4G4i3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wNgR_TVdG4zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CTVfBKtMHHvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nQ6BE-8SHIE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prioritize energy consumption in the deep reinforcement learning (DRL) setup while still considering the auxiliary objective (packet loss), we can use a weighted sum approach for the reward function. By assigning a higher weight to the energy consumption term, the algorithm will be more strongly incentivized to minimize energy consumption compared to packet loss.\n",
        "\n",
        "### Detailed Steps:\n",
        "\n",
        "1. **Define the Objectives**:\n",
        "   - **Main Objective (Energy Consumption)**: Minimize energy consumption (Joules).\n",
        "   - **Auxiliary Objective (Packet Loss)**: Minimize packet loss (percentage).\n",
        "\n",
        "2. **Normalization**:\n",
        "   - Normalize both energy consumption and packet loss to a common scale (e.g., between 0 and 1).\n",
        "\n",
        "3. **Weight Assignment**:\n",
        "   - Assign higher weight to the energy consumption objective to prioritize it over packet loss.\n",
        "\n",
        "4. **Composite Reward Function**:\n",
        "   - Construct a composite reward function that incorporates both normalized objectives with their respective weights.\n",
        "\n",
        "### Implementation:\n",
        "\n",
        "Here's an example implementation using a modified DDPG algorithm with a weighted composite reward function:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class MultiObjectiveDDPG:\n",
        "    def __init__(self, state_dim, action_dim, action_bound, weight_energy=0.7, weight_packet_loss=0.3):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_bound = action_bound\n",
        "        \n",
        "        # Weights for the objectives\n",
        "        self.weight_energy = weight_energy\n",
        "        self.weight_packet_loss = weight_packet_loss\n",
        "        \n",
        "        # Actor and Critic Networks\n",
        "        self.actor = self.build_actor()\n",
        "        self.critic = self.build_critic()\n",
        "        \n",
        "        # Target Networks\n",
        "        self.target_actor = self.build_actor()\n",
        "        self.target_critic = self.build_critic()\n",
        "        \n",
        "        # Optimizers\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "        \n",
        "        # Replay Buffer\n",
        "        self.buffer = []\n",
        "        self.buffer_size = 100000\n",
        "        self.batch_size = 64\n",
        "\n",
        "        # Min and Max values for normalization\n",
        "        self.min_energy = float('inf')\n",
        "        self.max_energy = float('-inf')\n",
        "        self.min_packet_loss = float('inf')\n",
        "        self.max_packet_loss = float('-inf')\n",
        "    \n",
        "    def build_actor(self):\n",
        "        state_input = layers.Input(shape=(self.state_dim,))\n",
        "        h = layers.Dense(256, activation='relu')(state_input)\n",
        "        h = layers.Dense(256, activation='relu')(h)\n",
        "        output = layers.Dense(self.action_dim, activation='tanh')(h)\n",
        "        scaled_output = layers.Lambda(lambda x: x * self.action_bound)(output)\n",
        "        return tf.keras.Model(state_input, scaled_output)\n",
        "    \n",
        "    def build_critic(self):\n",
        "        state_input = layers.Input(shape=(self.state_dim,))\n",
        "        action_input = layers.Input(shape=(self.action_dim,))\n",
        "        concat = layers.Concatenate()([state_input, action_input])\n",
        "        h = layers.Dense(256, activation='relu')(concat)\n",
        "        h = layers.Dense(256, activation='relu')(h)\n",
        "        output = layers.Dense(1)(h)\n",
        "        return tf.keras.Model([state_input, action_input], output)\n",
        "    \n",
        "    def update_target(self, tau=0.005):\n",
        "        # Update target networks\n",
        "        for target_param, param in zip(self.target_actor.trainable_variables, self.actor.trainable_variables):\n",
        "            target_param.assign(tau * param + (1 - tau) * target_param)\n",
        "        \n",
        "        for target_param, param in zip(self.target_critic.trainable_variables, self.critic.trainable_variables):\n",
        "            target_param.assign(tau * param + (1 - tau) * target_param)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        return self.actor.predict(state)[0]\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) > self.buffer_size:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def train(self):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        batch = np.random.choice(len(self.buffer), self.batch_size)\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*[self.buffer[i] for i in batch])\n",
        "        \n",
        "        state_batch = np.array(state_batch)\n",
        "        action_batch = np.array(action_batch)\n",
        "        reward_batch = np.array(reward_batch)\n",
        "        next_state_batch = np.array(next_state_batch)\n",
        "        done_batch = np.array(done_batch)\n",
        "        \n",
        "        # Critic update\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor(next_state_batch)\n",
        "            y = reward_batch + (1 - done_batch) * 0.99 * self.target_critic([next_state_batch, target_actions])\n",
        "            critic_value = self.critic([state_batch, action_batch])\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "        \n",
        "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
        "        \n",
        "        # Actor update\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor(state_batch)\n",
        "            critic_value = self.critic([state_batch, actions])\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "        \n",
        "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
        "        \n",
        "        # Update target networks\n",
        "        self.update_target()\n",
        "    \n",
        "    def composite_reward(self, energy_consumption, packet_loss):\n",
        "        normalized_energy = (energy_consumption - self.min_energy) / (self.max_energy - self.min_energy)\n",
        "        normalized_packet_loss = (packet_loss - self.min_packet_loss) / (self.max_packet_loss - self.min_packet_loss)\n",
        "        return self.weight_energy * normalized_energy + self.weight_packet_loss * normalized_packet_loss\n",
        "\n",
        "    def update_normalization_bounds(self, energy_consumption, packet_loss):\n",
        "        self.min_energy = min(self.min_energy, energy_consumption)\n",
        "        self.max_energy = max(self.max_energy, energy_consumption)\n",
        "        self.min_packet_loss = min(self.min_packet_loss, packet_loss)\n",
        "        self.max_packet_loss = max(self.max_packet_loss, packet_loss)\n",
        "\n",
        "# Example usage\n",
        "state_dim = 10\n",
        "action_dim = 2\n",
        "action_bound = 1.0\n",
        "agent = MultiObjectiveDDPG(state_dim, action_dim, action_bound, weight_energy=0.7, weight_packet_loss=0.3)\n",
        "\n",
        "# Training loop (simplified)\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, energy_consumption, packet_loss, done = env.step(action)\n",
        "        \n",
        "        # Update normalization bounds\n",
        "        agent.update_normalization_bounds(energy_consumption, packet_loss)\n",
        "        \n",
        "        reward = agent.composite_reward(energy_consumption, packet_loss)\n",
        "        agent.store_transition(state, action, reward, next_state, done)\n",
        "        agent.train()\n",
        "        state = next_state\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "1. **Define Objectives**:\n",
        "   - Main Objective: Energy consumption is considered the primary objective.\n",
        "   - Auxiliary Objective: Packet loss is considered the secondary objective.\n",
        "\n",
        "2. **Normalization**:\n",
        "   - Both energy consumption and packet loss are normalized to ensure comparability.\n",
        "\n",
        "3. **Weight Assignment**:\n",
        "   - The weights are defined such that energy consumption is given more priority (e.g., `weight_energy=0.7` and `weight_packet_loss=0.3`).\n",
        "\n",
        "4. **Composite Reward Function**:\n",
        "   - The `composite_reward` function combines both normalized objectives using the assigned weights. This ensures that energy consumption has a higher influence on the overall reward compared to packet loss.\n",
        "\n",
        "5. **Training**:\n",
        "   - The DDPG algorithm is modified to use the composite reward, balancing between energy consumption and packet loss, but prioritizing energy consumption due to the higher weight.\n",
        "\n",
        "### Conclusion:\n",
        "By assigning a higher weight to energy consumption in the composite reward function, the DRL algorithm is incentivized to prioritize minimizing energy consumption while still considering packet loss. This approach ensures that the optimization process focuses more on the primary objective while avoiding local optima by considering the auxiliary objective."
      ],
      "metadata": {
        "id": "bARXNeO0HIRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's outline a detailed implementation of your idea where we use energy consumption as the main objective and packet loss as the auxiliary objective in a deep reinforcement learning (DRL) setup for an IoT network. We'll define the reward function as a tuple of negative energy consumption and negative packet loss, normalize the rewards, and use a composite reward for training.\n",
        "\n",
        "### Step-by-Step Implementation\n",
        "\n",
        "1. **Define the Objectives**:\n",
        "   - **Main Objective (Energy Consumption)**: Minimize energy consumption (Joules).\n",
        "   - **Auxiliary Objective (Packet Loss)**: Minimize packet loss (percentage).\n",
        "\n",
        "2. **Reward Shaping**:\n",
        "   - Reward function returns a tuple of negative values for both energy consumption and packet loss.\n",
        "\n",
        "3. **Replay Buffer**:\n",
        "   - Store states, selected configurations, reward tuples, and next states.\n",
        "\n",
        "4. **Training with Composite Reward**:\n",
        "   - Normalize the reward values.\n",
        "   - Combine normalized rewards into a composite reward using a weighted sum.\n",
        "   - Train the agent using the composite reward.\n",
        "\n",
        "### Implementation\n",
        "\n",
        "Here's a complete implementation using TensorFlow and a modified DDPG algorithm:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class MultiObjectiveDDPG:\n",
        "    def __init__(self, state_dim, action_dim, action_bound, weight_energy=0.7, weight_packet_loss=0.3):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_bound = action_bound\n",
        "        \n",
        "        # Weights for the objectives\n",
        "        self.weight_energy = weight_energy\n",
        "        self.weight_packet_loss = weight_packet_loss\n",
        "        \n",
        "        # Actor and Critic Networks\n",
        "        self.actor = self.build_actor()\n",
        "        self.critic = self.build_critic()\n",
        "        \n",
        "        # Target Networks\n",
        "        self.target_actor = self.build_actor()\n",
        "        self.target_critic = self.build_critic()\n",
        "        \n",
        "        # Optimizers\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "        \n",
        "        # Replay Buffer\n",
        "        self.buffer = []\n",
        "        self.buffer_size = 100000\n",
        "        self.batch_size = 64\n",
        "\n",
        "        # Min and Max values for normalization\n",
        "        self.min_energy = float('inf')\n",
        "        self.max_energy = float('-inf')\n",
        "        self.min_packet_loss = float('inf')\n",
        "        self.max_packet_loss = float('-inf')\n",
        "    \n",
        "    def build_actor(self):\n",
        "        state_input = layers.Input(shape=(self.state_dim,))\n",
        "        h = layers.Dense(256, activation='relu')(state_input)\n",
        "        h = layers.Dense(256, activation='relu')(h)\n",
        "        output = layers.Dense(self.action_dim, activation='tanh')(h)\n",
        "        scaled_output = layers.Lambda(lambda x: x * self.action_bound)(output)\n",
        "        return tf.keras.Model(state_input, scaled_output)\n",
        "    \n",
        "    def build_critic(self):\n",
        "        state_input = layers.Input(shape=(self.state_dim,))\n",
        "        action_input = layers.Input(shape=(self.action_dim,))\n",
        "        concat = layers.Concatenate()([state_input, action_input])\n",
        "        h = layers.Dense(256, activation='relu')(concat)\n",
        "        h = layers.Dense(256, activation='relu')(h)\n",
        "        output = layers.Dense(1)(h)\n",
        "        return tf.keras.Model([state_input, action_input], output)\n",
        "    \n",
        "    def update_target(self, tau=0.005):\n",
        "        # Update target networks\n",
        "        for target_param, param in zip(self.target_actor.trainable_variables, self.actor.trainable_variables):\n",
        "            target_param.assign(tau * param + (1 - tau) * target_param)\n",
        "        \n",
        "        for target_param, param in zip(self.target_critic.trainable_variables, self.critic.trainable_variables):\n",
        "            target_param.assign(tau * param + (1 - tau) * target_param)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        return self.actor.predict(state)[0]\n",
        "    \n",
        "    def store_transition(self, state, action, reward_tuple, next_state, done):\n",
        "        if len(self.buffer) > self.buffer_size:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append((state, action, reward_tuple, next_state, done))\n",
        "    \n",
        "    def train(self):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        batch = np.random.choice(len(self.buffer), self.batch_size)\n",
        "        state_batch, action_batch, reward_tuple_batch, next_state_batch, done_batch = zip(*[self.buffer[i] for i in batch])\n",
        "        \n",
        "        state_batch = np.array(state_batch)\n",
        "        action_batch = np.array(action_batch)\n",
        "        reward_tuple_batch = np.array(reward_tuple_batch)\n",
        "        next_state_batch = np.array(next_state_batch)\n",
        "        done_batch = np.array(done_batch)\n",
        "        \n",
        "        # Normalize rewards\n",
        "        normalized_rewards = self.normalize_rewards(reward_tuple_batch)\n",
        "        \n",
        "        # Critic update\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor(next_state_batch)\n",
        "            y = normalized_rewards + (1 - done_batch) * 0.99 * self.target_critic([next_state_batch, target_actions])\n",
        "            critic_value = self.critic([state_batch, action_batch])\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "        \n",
        "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
        "        \n",
        "        # Actor update\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor(state_batch)\n",
        "            critic_value = self.critic([state_batch, actions])\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "        \n",
        "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
        "        \n",
        "        # Update target networks\n",
        "        self.update_target()\n",
        "    \n",
        "    def normalize_rewards(self, reward_tuple_batch):\n",
        "        energy_rewards = reward_tuple_batch[:, 0]\n",
        "        packet_loss_rewards = reward_tuple_batch[:, 1]\n",
        "        \n",
        "        self.min_energy = min(self.min_energy, np.min(energy_rewards))\n",
        "        self.max_energy = max(self.max_energy, np.max(energy_rewards))\n",
        "        self.min_packet_loss = min(self.min_packet_loss, np.min(packet_loss_rewards))\n",
        "        self.max_packet_loss = max(self.max_packet_loss, np.max(packet_loss_rewards))\n",
        "        \n",
        "        normalized_energy = (energy_rewards - self.min_energy) / (self.max_energy - self.min_energy)\n",
        "        normalized_packet_loss = (packet_loss_rewards - self.min_packet_loss) / (self.max_packet_loss - self.min_packet_loss)\n",
        "        \n",
        "        composite_rewards = self.weight_energy * normalized_energy + self.weight_packet_loss * normalized_packet_loss\n",
        "        return composite_rewards\n",
        "\n",
        "# Example usage\n",
        "state_dim = 10\n",
        "action_dim = 2\n",
        "action_bound = 1.0\n",
        "agent = MultiObjectiveDDPG(state_dim, action_dim, action_bound, weight_energy=0.7, weight_packet_loss=0.3)\n",
        "\n",
        "# Training loop (simplified)\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, energy_consumption, packet_loss, done = env.step(action)\n",
        "        \n",
        "        # Update normalization bounds\n",
        "        agent.update_normalization_bounds(energy_consumption, packet_loss)\n",
        "        \n",
        "        reward_tuple = (-energy_consumption, -packet_loss)  # Reward as tuple\n",
        "        agent.store_transition(state, action, reward_tuple, next_state, done)\n",
        "        agent.train()\n",
        "        state = next_state\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "1. **Define Objectives**:\n",
        "   - Main Objective: Minimize energy consumption.\n",
        "   - Auxiliary Objective: Minimize packet loss.\n",
        "\n",
        "2. **Reward Shaping**:\n",
        "   - The reward function returns a tuple with negative values of energy consumption and packet loss.\n",
        "\n",
        "3. **Replay Buffer**:\n",
        "   - Store state, selected configuration, reward tuple, and next state.\n",
        "\n",
        "4. **Normalization and Composite Reward**:\n",
        "   - The `normalize_rewards` function normalizes the rewards for energy consumption and packet loss.\n",
        "   - The composite reward is calculated using a weighted sum of the normalized rewards.\n",
        "\n",
        "5. **Training**:\n",
        "   - During training, a batch is sampled from the replay buffer.\n",
        "   - Rewards are normalized and combined into a composite reward.\n",
        "   - The actor and critic networks are updated based on the composite reward.\n",
        "\n",
        "### Conclusion:\n",
        "By defining the reward function as a tuple of negative energy consumption and negative packet loss, and normalizing these values to form a composite reward, the agent is incentivized to prioritize minimizing energy consumption while also considering packet loss. This approach leverages multi-objective optimization principles within the DRL framework, ensuring that the main objective is prioritized while still avoiding local optima through consideration of the auxiliary objective."
      ],
      "metadata": {
        "id": "24bZi1mlHVNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's work through a detailed numerical example of how to apply the multi-objective deep reinforcement learning approach with energy consumption as the main objective and packet loss as the auxiliary objective. We'll use the principles of normalization and composite rewards as outlined in the previous response.\n",
        "\n",
        "### Scenario:\n",
        "Imagine we have an IoT network with configurations resulting in different levels of energy consumption and packet loss. We aim to train an agent that minimizes both, with a higher priority on minimizing energy consumption.\n",
        "\n",
        "### Step-by-Step Numerical Example:\n",
        "\n",
        "#### 1. Initial Setup\n",
        "Let's consider three configurations with the following results:\n",
        "\n",
        "| Configuration | Energy Consumption (Joules) | Packet Loss (%) |\n",
        "|---------------|------------------------------|------------------|\n",
        "| A             | 15                           | 4                |\n",
        "| B             | 10                           | 6                |\n",
        "| C             | 12                           | 3                |\n",
        "\n",
        "#### 2. Define the Reward Function\n",
        "The reward function returns a tuple with negative values for both energy consumption and packet loss:\n",
        "\n",
        "- Reward for Configuration A: (-15, -4)\n",
        "- Reward for Configuration B: (-10, -6)\n",
        "- Reward for Configuration C: (-12, -3)\n",
        "\n",
        "#### 3. Store in Replay Buffer\n",
        "The replay buffer stores states, actions, reward tuples, and next states. For simplicity, let's assume:\n",
        "- States are the initial states of the network.\n",
        "- Actions are the configurations selected.\n",
        "- Next states are the resulting states after applying the configurations.\n",
        "\n",
        "For example:\n",
        "- Initial state: `S0`\n",
        "- Next state for A: `S1`\n",
        "- Next state for B: `S2`\n",
        "- Next state for C: `S3`\n",
        "\n",
        "#### 4. Normalization of Rewards\n",
        "Normalize the rewards based on the min-max normalization method. We need to determine the min and max values of energy consumption and packet loss:\n",
        "\n",
        "- Min Energy: 10\n",
        "- Max Energy: 15\n",
        "- Min Packet Loss: 3\n",
        "- Max Packet Loss: 6\n",
        "\n",
        "Normalized values are calculated as follows:\n",
        "\\[\n",
        "\\text{Normalized Value} = \\frac{\\text{Value} - \\text{Min}}{\\text{Max} - \\text{Min}}\n",
        "\\]\n",
        "\n",
        "For Energy Consumption:\n",
        "- Normalized Energy for A: \\(\\frac{15 - 10}{15 - 10} = 1.0\\)\n",
        "- Normalized Energy for B: \\(\\frac{10 - 10}{15 - 10} = 0.0\\)\n",
        "- Normalized Energy for C: \\(\\frac{12 - 10}{15 - 10} = 0.4\\)\n",
        "\n",
        "For Packet Loss:\n",
        "- Normalized Packet Loss for A: \\(\\frac{4 - 3}{6 - 3} = 0.333\\)\n",
        "- Normalized Packet Loss for B: \\(\\frac{6 - 3}{6 - 3} = 1.0\\)\n",
        "- Normalized Packet Loss for C: \\(\\frac{3 - 3}{6 - 3} = 0.0\\)\n",
        "\n",
        "#### 5. Composite Reward Calculation\n",
        "Assume the weights for the objectives are:\n",
        "- Weight for Energy Consumption: \\(0.7\\)\n",
        "- Weight for Packet Loss: \\(0.3\\)\n",
        "\n",
        "Composite Reward:\n",
        "\\[\n",
        "\\text{Composite Reward} = \\text{Weight Energy} \\times \\text{Normalized Energy} + \\text{Weight Packet Loss} \\times \\text{Normalized Packet Loss}\n",
        "\\]\n",
        "\n",
        "Calculate the composite reward for each configuration:\n",
        "\n",
        "- For Configuration A:\n",
        "  \\[\n",
        "  \\text{Composite Reward} = 0.7 \\times 1.0 + 0.3 \\times 0.333 = 0.7 + 0.1 = 0.8\n",
        "  \\]\n",
        "\n",
        "- For Configuration B:\n",
        "  \\[\n",
        "  \\text{Composite Reward} = 0.7 \\times 0.0 + 0.3 \\times 1.0 = 0.0 + 0.3 = 0.3\n",
        "  \\]\n",
        "\n",
        "- For Configuration C:\n",
        "  \\[\n",
        "  \\text{Composite Reward} = 0.7 \\times 0.4 + 0.3 \\times 0.0 = 0.28 + 0.0 = 0.28\n",
        "  \\]\n",
        "\n",
        "#### 6. Update Replay Buffer with Composite Rewards\n",
        "Store the states, actions, composite rewards, and next states in the replay buffer. For example:\n",
        "\n",
        "| State | Action | Composite Reward | Next State |\n",
        "|-------|--------|------------------|------------|\n",
        "| S0    | A      | 0.8              | S1         |\n",
        "| S0    | B      | 0.3              | S2         |\n",
        "| S0    | C      | 0.28             | S3         |\n",
        "\n",
        "#### 7. Sample a Batch and Train the Agent\n",
        "Sample a batch from the replay buffer and use the composite rewards for training the actor and critic networks.\n",
        "\n",
        "### Conclusion\n",
        "By following these steps, we have integrated the MMO approach into a DRL setup. This ensures the agent prioritizes minimizing energy consumption while also considering packet loss. The composite reward provides a balanced incentive structure, guiding the agent towards optimal configurations in the IoT network.\n",
        "\n",
        "### Example Code for Normalization and Composite Reward Calculation\n",
        "\n",
        "```python\n",
        "def normalize_rewards(reward_tuples, min_energy, max_energy, min_packet_loss, max_packet_loss):\n",
        "    normalized_rewards = []\n",
        "    for reward in reward_tuples:\n",
        "        energy, packet_loss = reward\n",
        "        \n",
        "        normalized_energy = (energy - min_energy) / (max_energy - min_energy)\n",
        "        normalized_packet_loss = (packet_loss - min_packet_loss) / (max_packet_loss - min_packet_loss)\n",
        "        \n",
        "        normalized_rewards.append((normalized_energy, normalized_packet_loss))\n",
        "    return normalized_rewards\n",
        "\n",
        "def calculate_composite_rewards(normalized_rewards, weight_energy, weight_packet_loss):\n",
        "    composite_rewards = []\n",
        "    for reward in normalized_rewards:\n",
        "        normalized_energy, normalized_packet_loss = reward\n",
        "        composite_reward = (weight_energy * normalized_energy) + (weight_packet_loss * normalized_packet_loss)\n",
        "        composite_rewards.append(composite_reward)\n",
        "    return composite_rewards\n",
        "\n",
        "# Example rewards\n",
        "rewards = [(-15, -4), (-10, -6), (-12, -3)]\n",
        "\n",
        "# Min and max values for normalization\n",
        "min_energy, max_energy = 10, 15\n",
        "min_packet_loss, max_packet_loss = 3, 6\n",
        "\n",
        "# Normalize rewards\n",
        "normalized_rewards = normalize_rewards(rewards, min_energy, max_energy, min_packet_loss, max_packet_loss)\n",
        "print(\"Normalized Rewards:\", normalized_rewards)\n",
        "\n",
        "# Calculate composite rewards\n",
        "weight_energy = 0.7\n",
        "weight_packet_loss = 0.3\n",
        "composite_rewards = calculate_composite_rewards(normalized_rewards, weight_energy, weight_packet_loss)\n",
        "print(\"Composite Rewards:\", composite_rewards)\n",
        "```\n",
        "\n",
        "### Output:\n",
        "```\n",
        "Normalized Rewards: [(1.0, 0.3333333333333333), (0.0, 1.0), (0.4, 0.0)]\n",
        "Composite Rewards: [0.7999999999999999, 0.3, 0.27999999999999997]\n",
        "```\n",
        "\n",
        "This numerical example and code illustrate how to apply the MMO principles in a DRL setup, prioritizing energy consumption while considering packet loss."
      ],
      "metadata": {
        "id": "ikJU-rZ1HVeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jDAks4IKHVwk"
      }
    }
  ]
}