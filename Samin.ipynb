{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtPMx5LO3pimJG63vbyALc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezakavianifar/RL-DeltaIoT/blob/main/Samin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj87b1lzfSGO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main novelty of the article \"Decision-making Under Uncertainty: Be Aware of Your Priorities\" lies in the introduction of the Pri-AwaRE architecture. This architecture uses an extended form of the Multi-Reward Partially Observable Markov Decision Process (MR-POMDP++), integrated into the MAPE-K loop, to support priority-aware decision-making in self-adaptive systems (SASs). The key contributions include:\n",
        "\n",
        "1. **Priority-aware Decision-Making**: The architecture models and reasons about the priorities of individual non-functional requirements (NFRs) using a vector-valued reward function. This allows the system to re-evaluate and adjust priorities based on new knowledge acquired during runtime.\n",
        "\n",
        "2. **Autonomous Tuning of Priorities**: The system provides a method for maintaining compliance with requirements by autonomously tuning NFR priorities in response to uncertain environmental contexts.\n",
        "\n",
        "3. **Experimental Validation**: The approach is validated through experiments in the networking and IoT domains, demonstrating that the Pri-AwaRE architecture leads to better satisfaction of NFRs through more informed priority choices【9:0†source】【9:3†source】."
      ],
      "metadata": {
        "id": "oG0DYfsLfTkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The article extends the Multi-Reward Partially Observable Markov Decision Process (MR-POMDP) by introducing the MR-POMDP++ framework. The enhancements include:\n",
        "\n",
        "1. **Vector-valued Reward Function**: Unlike traditional MR-POMDPs, which use a vector to represent rewards for multiple objectives, the MR-POMDP++ incorporates these reward values into the decision-making process more dynamically by modeling the priorities of non-functional requirements (NFRs) at runtime.\n",
        "\n",
        "2. **Alpha-Matrix Representation**: In MR-POMDP++, each element in the alpha vector is itself a vector, resulting in an alpha matrix. This matrix provides a more detailed representation of the value function, capturing the multi-objective nature of the problem more effectively.\n",
        "\n",
        "3. **Scalarization Function**: The framework uses a scalarization function to select the best policy among multiple optimal policies based on the different priorities of the objectives. This function combines the value vectors with weights corresponding to the objectives, allowing the system to adapt the priorities of these objectives at runtime using the Optimistic Linear Support (OLS) algorithm.\n",
        "\n",
        "4. **Runtime Autonomous Tuning**: The architecture enables autonomous tuning of NFR priorities during runtime, ensuring that the system can adapt to changing environmental contexts and maintain compliance with requirements   ."
      ],
      "metadata": {
        "id": "O9SqMsaKfYfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, let's use a simplified example to illustrate the differences between MR-POMDPs and MR-POMDP++.\n",
        "\n",
        "### Example Scenario\n",
        "Imagine a self-adaptive Internet of Things (IoT) network in a smart home. The system has two main non-functional requirements (NFRs):\n",
        "1. **Energy Efficiency (EE)**: The system should minimize energy consumption.\n",
        "2. **Packet Delivery Ratio (PDR)**: The system should maximize the successful delivery of packets.\n",
        "\n",
        "The environment is dynamic and can vary between low and high interference conditions, which affect both EE and PDR.\n",
        "\n",
        "### MR-POMDP\n",
        "In a traditional MR-POMDP:\n",
        "1. **State Representation**: The states might represent different levels of interference (e.g., low, medium, high).\n",
        "2. **Action Set**: Actions could include adjusting the transmission power (e.g., low power, medium power, high power).\n",
        "3. **Reward Function**: The rewards for each action in each state are represented as a vector. For instance, increasing transmission power might have the reward vector [EE = -1 (high energy consumption), PDR = 2 (high packet delivery)].\n",
        "\n",
        "Here’s how MR-POMDP would handle it:\n",
        "- **Decision-Making**: The system uses these vectors to decide the best action by looking at the trade-offs between EE and PDR. It will calculate a policy that maximizes the cumulative reward for both objectives.\n",
        "- **Static Priorities**: The priorities (weights) for EE and PDR are fixed and determined at design time. For example, it might prioritize PDR slightly higher than EE.\n",
        "\n",
        "### MR-POMDP++\n",
        "In MR-POMDP++:\n",
        "1. **Enhanced State Representation**: Similar to MR-POMDP, but with additional mechanisms to capture runtime information.\n",
        "2. **Dynamic Reward Adjustment**: The system dynamically adjusts the priorities of EE and PDR based on the current context and historical data.\n",
        "\n",
        "Here’s how MR-POMDP++ would handle it:\n",
        "- **Priority-Aware Decision-Making**: The system uses a vector-valued reward function, but it also incorporates a mechanism to adjust the priorities of EE and PDR dynamically. For example, if the system detects that the battery level is low, it might increase the priority of EE.\n",
        "- **Alpha-Matrix**: The alpha vectors are now matrices where each element is a vector, providing a more nuanced decision-making process that can capture the changing priorities of NFRs.\n",
        "- **Runtime Autonomous Tuning**: If the interference level changes from low to high, MR-POMDP++ can autonomously adjust the weights given to EE and PDR. For instance, during high interference, it might decide that maintaining a higher PDR is more critical even if it means higher energy consumption.\n",
        "\n",
        "### Detailed Example\n",
        "- **State**: `s1` (low interference), `s2` (high interference)\n",
        "- **Action**: `a1` (low power), `a2` (high power)\n",
        "\n",
        "#### MR-POMDP:\n",
        "- **Rewards for `a1` in `s1`**: [EE = 2, PDR = 1]\n",
        "- **Rewards for `a2` in `s1`**: [EE = -1, PDR = 3]\n",
        "- **Fixed Priority**: Priorities might be set as [EE: 0.4, PDR: 0.6]\n",
        "\n",
        "**Policy Decision**: Always choose `a2` (high power) because it maximizes the combined reward considering fixed priorities.\n",
        "\n",
        "#### MR-POMDP++:\n",
        "- **Rewards for `a1` in `s1`**: [EE = 2, PDR = 1]\n",
        "- **Rewards for `a2` in `s1`**: [EE = -1, PDR = 3]\n",
        "- **Dynamic Priority Adjustment**: If the battery level is low, the system adjusts the priorities dynamically to [EE: 0.7, PDR: 0.3]\n",
        "\n",
        "**Policy Decision**: Depending on the battery level:\n",
        "- **Normal Battery**: Choose `a2` (high power) to maximize PDR.\n",
        "- **Low Battery**: Choose `a1` (low power) to conserve energy.\n",
        "\n",
        "### Key Differences\n",
        "- **MR-POMDP**: Uses fixed priorities and combines rewards into a single scalar value for decision-making.\n",
        "- **MR-POMDP++**: Allows for dynamic adjustment of priorities based on runtime conditions and uses a more sophisticated alpha-matrix to represent the value functions for decision-making, supporting autonomous adaptation to changing environments.\n",
        "\n",
        "This enhanced capability allows MR-POMDP++ to better handle uncertainty and variability in the environment, providing more robust and context-aware decision-making for self-adaptive systems."
      ],
      "metadata": {
        "id": "8aAObLc8fbIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An alpha matrix in the context of the MR-POMDP++ framework is an advanced representation of the value function used for decision-making under uncertainty with multiple objectives. This matrix extends the traditional alpha vectors used in POMDPs to handle multiple reward components and dynamically adjust priorities during runtime.\n",
        "\n",
        "### Traditional Alpha Vectors\n",
        "In a standard POMDP, the value function for a policy is represented by a set of alpha vectors. Each alpha vector corresponds to a particular belief state and is used to estimate the expected reward for that state.\n",
        "\n",
        "### Alpha Matrix in MR-POMDP++\n",
        "In MR-POMDP++, each element of the alpha vector is itself a vector, resulting in an alpha matrix. This structure allows the system to account for multiple objectives and dynamically adjust their priorities based on the current context.\n",
        "\n",
        "### Example Scenario\n",
        "Let's consider a simplified example with the following components:\n",
        "\n",
        "- **States**: \\(s1\\), \\(s2\\) (e.g., low and high interference levels in a network).\n",
        "- **Actions**: \\(a1\\), \\(a2\\) (e.g., low power and high power transmission).\n",
        "- **Objectives**: EE (Energy Efficiency) and PDR (Packet Delivery Ratio).\n",
        "\n",
        "#### Traditional MR-POMDP Alpha Vectors\n",
        "In a traditional MR-POMDP, you might have reward vectors for each action in each state, such as:\n",
        "- For \\(a1\\) in \\(s1\\): \\([EE = 2, PDR = 1]\\)\n",
        "- For \\(a2\\) in \\(s1\\): \\([EE = -1, PDR = 3]\\)\n",
        "\n",
        "The alpha vector for a belief state could look like:\n",
        "\\[ \\alpha_{s1} = [2, -1] \\]\n",
        "\n",
        "#### MR-POMDP++ Alpha Matrix\n",
        "In MR-POMDP++, the alpha matrix extends this by incorporating priority-aware adjustments. Suppose we have a priority vector \\([w_{EE}, w_{PDR}]\\) that adjusts based on runtime conditions.\n",
        "\n",
        "1. **Alpha Vector for State \\(s1\\) with Action \\(a1\\)**:\n",
        "\\[ \\alpha_{s1, a1} =\n",
        "\\begin{bmatrix}\n",
        "2 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "2. **Alpha Vector for State \\(s1\\) with Action \\(a2\\)**:\n",
        "\\[ \\alpha_{s1, a2} =\n",
        "\\begin{bmatrix}\n",
        "-1 \\\\\n",
        "3\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "These vectors form part of the alpha matrix for state \\(s1\\):\n",
        "\\[ \\alpha_{s1} =\n",
        "\\begin{bmatrix}\n",
        "2 & -1 \\\\\n",
        "1 & 3\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "### Dynamic Adjustment\n",
        "If the system detects a low battery, it might adjust the priority vector to \\([w_{EE} = 0.7, w_{PDR} = 0.3]\\). The scalarization function combines these weights with the alpha matrix to determine the best action:\n",
        "\n",
        "For state \\(s1\\):\n",
        "\\[ \\alpha_{s1} \\cdot \\text{priority vector} =\n",
        "\\begin{bmatrix}\n",
        "2 & -1 \\\\\n",
        "1 & 3\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "0.7 \\\\\n",
        "0.3\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "(2 \\cdot 0.7) + (-1 \\cdot 0.3) \\\\\n",
        "(1 \\cdot 0.7) + (3 \\cdot 0.3)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1.4 - 0.3 \\\\\n",
        "0.7 + 0.9\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1.1 \\\\\n",
        "1.6\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "Here, the scalarized values would be used to select the optimal action considering the adjusted priorities. The system dynamically adapts the decision based on current priorities, offering more context-aware and effective decision-making compared to the static approach of traditional MR-POMDPs.\n",
        "\n",
        "### Summary\n",
        "The alpha matrix in MR-POMDP++ allows for:\n",
        "- **Multi-objective Handling**: Managing multiple objectives simultaneously with dynamic adjustments.\n",
        "- **Priority-Aware Decision-Making**: Adjusting the importance of different objectives based on runtime conditions.\n",
        "- **Enhanced Value Representation**: Providing a more detailed and flexible representation of the value function, improving decision-making under uncertainty."
      ],
      "metadata": {
        "id": "mhlxEzzffjwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "08bWEslbfgMv"
      }
    }
  ]
}